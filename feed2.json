[
 {
  "title": "RICK",
  "content": "I am wired to constantly ask \u201cwhat\u2019s next?\u201d\u00a0Sometimes, the answer is: \u201cmore of the same.\u201d\nThat came to mind when a friend raised a point about emerging technology\u2019s fractal nature. Across one story arc, they said, we often see several structural evolutions\u2014smaller-scale versions of that wider phenomenon.\nCloud computing? It progressed from \u201craw compute and storage\u201d to \u201creimplementing key services in push-button fashion\u201d to \u201cbecoming the backbone of AI work\u201d\u2014all under the umbrella of \u201crenting time and storage on someone else\u2019s computers.\u201d Web3 has similarly progressed through \u201cbasic blockchain and cryptocurrency tokens\u201d to \u201cdecentralized finance\u201d to \u201cNFTs as loyalty cards.\u201d Each step has been a twist on \u201cwhat if we could write code to interact with a tamper-resistant ledger in real-time?\u201d\nMost recently, I\u2019ve been thinking about this in terms of the space we currently call \u201cAI.\u201d I\u2019ve called out the data field\u2019s rebranding efforts before; but even then, I acknowledged that these weren\u2019t just new coats of paint. Each time, the underlying implementation changed a bit while still staying true to the larger phenomenon of \u201cAnalyzing Data for Fun and Profit.\u201d\nConsider the structural evolutions of that theme:\nStage 1: Hadoop and Big Data \nBy 2008, many companies found themselves at the intersection of \u201ca steep increase in online activity\u201d and \u201ca sharp decline in costs for storage and computing.\u201d They weren\u2019t quite sure what this \u201cdata\u201d substance was, but they\u2019d convinced themselves that they had tons of it that they could monetize. All they needed was a tool that could handle the massive workload. And Hadoop rolled in.\nIn short order, it was tough to get a data job if you didn\u2019t have some Hadoop behind your name. And harder to sell a data-related product unless it spoke to Hadoop. The elephant was unstoppable.\nUntil it wasn\u2019t.\u00a0\nHadoop\u2019s value\u2014being able to crunch large datasets\u2014often paled in comparison to its costs. A basic, production-ready cluster priced out to the low-six-figures. A company then needed to train up their ops team to manage the cluster, and their analysts to express their ideas in MapReduce. Plus there was all of the infrastructure to push data into the cluster in the first place. \nIf you weren\u2019t in the terabytes-a-day club, you really had to take a step back and ask what this was all for. Doubly so as hardware improved, eating away at the lower end of Hadoop-worthy work.\nAnd then there was the other problem: for all the fanfare, Hadoop was really large-scale business intelligence (BI).\n(Enough time has passed; I think we can now be honest with ourselves. We built an entire industry by \u2026 repackaging an existing industry. This is the power of marketing.)\nDon\u2019t get me wrong. BI is useful. I\u2019ve sung its praises time and again. But the grouping and summarizing just wasn\u2019t exciting enough for the data addicts. They\u2019d grown tired of learning what is; now they wanted to know what\u2019s next.\nStage 2: Machine learning models\nHadoop could kind of do ML, thanks to third-party tools. But in its early form of a Hadoop-based ML library, Mahout still required data scientists to write in Java. And it (wisely) stuck to implementations of industry-standard algorithms. If you wanted ML beyond what Mahout provided, you had to frame your problem in MapReduce terms. Mental contortions led to code contortions led to frustration. And, often, to giving up.\n(After coauthoring Parallel R I gave a number of talks on using Hadoop. A common audience question was \u201ccan Hadoop run [my arbitrary analysis job or home-grown algorithm]?\u201d And my answer was a qualified yes: \u201cHadoop could theoretically scale your job. But only if you or someone else will take the time to implement that approach in MapReduce.\u201d That didn\u2019t go over well.)\nGoodbye, Hadoop. Hello, R and scikit-learn. A typical data job interview now skipped MapReduce in favor of white-boarding k-means clustering or random forests.\nAnd it was good. For a few years, even. But then we hit another hurdle.\nWhile data scientists were no longer handling Hadoop-sized workloads, they were trying to build predictive models on a different kind of \u201clarge\u201d dataset: so-called \u201cunstructured data.\u201d (I prefer to call that \u201csoft numbers,\u201d but that\u2019s another story.) A single document may represent thousands of features. An image? Millions.\nSimilar to the dawn of Hadoop, we were back to problems that existing tools could not solve.\nThe solution led us to the next structural evolution. And that brings our story to the present day:\nStage 3: Neural networks\nHigh-end video games required high-end video cards. And since the cards couldn\u2019t tell the difference between \u201cmatrix algebra for on-screen display\u201d and \u201cmatrix algebra for machine learning,\u201d neural networks became computationally feasible and commercially viable. It felt like, almost overnight, all of machine learning took on some kind of neural backend. Those algorithms packaged with scikit-learn? They were unceremoniously relabeled \u201cclassical machine learning.\u201d\nThere\u2019s as much Keras, TensorFlow, and Torch today as there was Hadoop back in 2010-2012. The data scientist\u2014sorry, \u201cmachine learning engineer\u201d or \u201cAI specialist\u201d\u2014job interview now involves one of those toolkits, or one of the higher-level abstractions such as HuggingFace Transformers.\nAnd just as we started to complain that the crypto miners were snapping up all of the affordable GPU cards, cloud providers stepped up to offer access on-demand. Between Google (Vertex AI and Colab) and Amazon (SageMaker), you can now get all of the GPU power your credit card can handle. Google goes a step further in offering compute instances with its specialized TPU hardware.\nNot that you\u2019ll even need GPU access all that often. A number of groups, from small research teams to tech behemoths, have used their own GPUs to train on large, interesting datasets and they give those models away for free on sites like TensorFlow Hub and Hugging Face Hub. You can download these models to use out of the box, or employ minimal compute resources to fine-tune them for your particular task.\nYou see the extreme version of this pretrained model phenomenon in the large language models (LLMs) that drive tools like Midjourney or ChatGPT. The overall idea of generative AI is to get a model to create content that could have reasonably fit into its training data. For a sufficiently large training dataset\u2014say, \u201cbillions of online images\u201d or \u201cthe entirety of Wikipedia\u201d\u2014a model can pick up on the kinds of patterns that make its outputs seem eerily lifelike.\nSince we\u2019re covered as far as compute power, tools, and even prebuilt models, what are the frictions of GPU-enabled ML? What will drive us to the next structural iteration of Analyzing Data for Fun and Profit?\nStage 4? Simulation\nGiven the progression thus far, I think the next structural evolution of Analyzing Data for Fun and Profit will involve a new appreciation for randomness. Specifically, through simulation.\nYou can see a simulation as a temporary, synthetic environment in which to test an idea. We do this all the time, when we ask \u201cwhat if?\u201d and play it out in our minds. \u201cWhat if we leave an hour earlier?\u201d\u00a0(We\u2019ll miss rush hour traffic.) \u201cWhat if I bring my duffel bag instead of the roll-aboard?\u201d (It will be easier to fit in the overhead storage.) That works just fine when there are only a few possible outcomes, across a small set of parameters.\nOnce we\u2019re able to quantify a situation, we can let a computer run \u201cwhat if?\u201d scenarios at industrial scale. Millions of tests, across as many parameters as will fit on the hardware. It\u2019ll even summarize the results if we ask nicely. That opens the door to a number of possibilities, three of which I\u2019ll highlight here:\nMoving beyond from point estimates\nLet\u2019s say an ML model tells us that this house should sell for $744,568.92. Great! We\u2019ve gotten a machine to make a prediction for us. What more could we possibly want?\nContext, for one. The model\u2019s output is just a single number, a point estimate of the most likely price. What we really want is the spread\u2014the range of likely values for that price. Does the model think the correct price falls between $743k-$746k? Or is it more like $600k-$900k? You want the former case if you\u2019re trying to buy or sell that property.\nBayesian data analysis, and other techniques that rely on simulation behind the scenes, offer additional insight here. These approaches vary some parameters, run the process a few million times, and give us a nice curve that shows how often the answer is (or, \u201cis not\u201d) close to that $744k.\nSimilarly, Monte Carlo simulations can help us spot trends and outliers in potential outcomes of a process. \u201cHere\u2019s our risk model. Let\u2019s assume these ten parameters can vary, then try the model with several million variations on those parameter sets. What can we learn about the potential outcomes?\u201d Such a simulation could reveal that, under certain specific circumstances, we get a case of total ruin. Isn\u2019t it nice to uncover that in a simulated environment, where we can map out our risk mitigation strategies with calm, level heads?\nMoving beyond point estimates is very close to present-day AI challenges. That\u2019s why it\u2019s a likely next step in Analyzing Data for Fun and Profit. In turn, that could open the door to other techniques:\nNew ways of exploring the solution space\nIf you\u2019re not familiar with evolutionary algorithms, they\u2019re a twist on the traditional Monte Carlo approach. In fact, they\u2019re like several small Monte Carlo simulations run in sequence. After each iteration, the process compares the results to its fitness function, then mixes the attributes of the top performers. Hence the term \u201cevolutionary\u201d\u2014combining the winners is akin to parents passing a mix of their attributes on to progeny. Repeat this enough times and you may just find the best set of parameters for your problem.\n(People familiar with optimization algorithms will recognize this as a twist on simulated annealing: start with random parameters and attributes, and narrow that scope over time.)\nA number of scholars have tested this shuffle-and-recombine-till-we-find-a-winner approach on timetable scheduling. Their research has applied evolutionary algorithms to groups that need efficient ways to manage finite, time-based resources such as classrooms and factory equipment. Other groups have tested evolutionary algorithms in drug discovery. Both situations benefit from a technique that optimizes the search through a large and daunting solution space.\nThe NASA ST5 antenna is another example. Its bent, twisted wire stands in stark contrast to the straight aerials with which we are familiar. There\u2019s no chance that a human would ever have come up with it.\u00a0But the evolutionary approach could, in part because it was not limited by human sense of aesthetic or any preconceived notions of what an \u201cantenna\u201d could be. It just kept shuffling the designs that satisfied its fitness function until the process finally converged.\nTaming complexity\nComplex adaptive systems are hardly a new concept, though most people got a harsh introduction at the start of the Covid-19 pandemic. Cities closed down, supply chains snarled, and people\u2014independent actors, behaving in their own best interests\u2014made it worse by hoarding supplies because they thought distribution and manufacturing would never recover. Today, reports of idle cargo ships and overloaded seaside ports remind us that we shifted from under- to over-supply. The mess is far from over.\nWhat makes a complex system troublesome isn\u2019t the sheer number of connections. It\u2019s not even that many of those connections are invisible because a person can\u2019t see the entire system at once. The problem is that those hidden connections only become visible during a malfunction: a failure in Component B affects not only neighboring Components A and C, but also triggers disruptions in T and R. R\u2019s issue is small on its own, but it has just led to an outsized impact in \u03a6 and \u03a3. \n(And if you just asked \u201cwait, how did Greek letters get mixed up in this?\u201d then \u2026\u00a0 you get the point.)\nOur current crop of AI tools is powerful, yet ill-equipped to provide insight into complex systems. We can\u2019t surface these hidden connections using a collection of independently-derived point estimates; we need something that can simulate the entangled system of independent actors moving all at once.\nThis is where agent-based modeling (ABM) comes into play. This technique simulates interactions in a complex system. Similar to the way a Monte Carlo simulation can surface outliers, an ABM can catch unexpected or unfavorable interactions in a safe, synthetic environment.\nFinancial markets and other economic situations are prime candidates for ABM. These are spaces where a large number of actors behave according to their rational self-interest, and their actions feed into the system and affect others\u2019 behavior. According to practitioners of complexity economics (a study that owes its origins to the Sante Fe Institute), traditional economic modeling treats these systems as though they run in an equilibrium state and therefore fails to identify certain kinds of disruptions. ABM captures a more realistic picture because it simulates a system that feeds back into itself.\nSmoothing the on-ramp\nInterestingly enough, I haven\u2019t mentioned anything new or ground-breaking. Bayesian data analysis and Monte Carlo simulations are common in finance and insurance. I was first introduced to evolutionary algorithms and agent-based modeling more than fifteen years ago. (If memory serves, this was shortly before I shifted my career to what we now call AI.) And even then I was late to the party.\nSo why hasn\u2019t this next phase of Analyzing Data for Fun and Profit taken off?\nFor one, this structural evolution needs a name. Something to distinguish it from \u201cAI.\u201d Something to market. I\u2019ve been using the term \u201csynthetics,\u201d so I\u2019ll offer that up. (Bonus: this umbrella term neatly includes generative AI\u2019s ability to create text, images, and other realistic-yet-heretofore-unseen data points. So we can ride that wave of publicity.)\nNext up is compute power. Simulations are CPU-heavy, and sometimes memory-bound. Cloud computing providers make that easier to handle, though, so long as you don\u2019t mind the credit card bill. Eventually we\u2019ll get simulation-specific hardware\u2014what will be the GPU or TPU of simulation?\u2014but I think synthetics can gain traction on existing gear.\nThe third and largest hurdle is the lack of simulation-specific frameworks. As we surface more use cases\u2014as we apply these techniques to real business problems or even academic challenges\u2014we\u2019ll improve the tools because we\u2019ll want to make that work easier. As the tools improve, that reduces the costs of trying the techniques on other use cases. This kicks off another iteration of the value loop. Use cases tend to magically appear as techniques get easier to use.\nIf you think I\u2019m overstating the power of tools to spread an idea, imagine trying to solve a problem with a new toolset while also creating that toolset at the same time. It\u2019s tough to balance those competing concerns. If someone else offers to build the tool while you use it and road-test it, you\u2019re probably going to accept. This is why these days we use TensorFlow or Torch instead of hand-writing our backpropagation loops.\nToday\u2019s landscape of simulation tooling is uneven. People doing Bayesian data analysis have their choice of two robust, authoritative offerings in Stan and PyMC3, plus a variety of books to understand the mechanics of the process. Things fall off after that. Most of the Monte Carlo simulations I\u2019ve seen are of the hand-rolled variety. And a quick survey of agent-based modeling and evolutionary algorithms turns up a mix of proprietary apps and nascent open-source projects, some of which are geared for a particular problem domain.\nAs we develop the authoritative toolkits for simulations\u2014the TensorFlow of agent-based modeling and the Hadoop of evolutionary algorithms, if you will\u2014expect adoption to grow. Doubly so, as commercial entities build services around those toolkits and rev up their own marketing (and publishing, and certification) machines. \nTime will tell \nMy expectations of what to come are, admittedly, shaped by my experience and clouded by my interests. Time will tell whether any of this hits the mark.\nA change in business or consumer appetite could also send the field down a different road. The next hot device, app, or service will get an outsized vote in what companies and consumers expect of technology.\nStill, I see value in looking for this field\u2019s structural evolutions. The wider story arc changes with each iteration to address changes in appetite. Practitioners and entrepreneurs, take note.\nJob-seekers should do the same. Remember that you once needed Hadoop on your r\u00e9sum\u00e9 to merit a second look; nowadays it\u2019s a liability.\u00a0Building models is a desired skill for now, but it\u2019s slowly giving way to robots.\u00a0So do you really think it\u2019s too late to join the data field? I think not.\nKeep an eye out for that next wave. That\u2019ll be your time to jump in.",
  "link": "https://www.oreilly.com/radar/structural-evolutions-in-data/"
 }
]