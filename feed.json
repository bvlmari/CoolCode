[
 {
  "title": "Structural Evolutions in Data",
  "content": "I am wired to constantly ask \u201cwhat\u2019s next?\u201d\u00a0Sometimes, the answer is: \u201cmore of the same.\u201d\nThat came to mind when a friend raised a point about emerging technology\u2019s fractal nature. Across one story arc, they said, we often see several structural evolutions\u2014smaller-scale versions of that wider phenomenon.\nCloud computing? It progressed from \u201craw compute and storage\u201d to \u201creimplementing key services in push-button fashion\u201d to \u201cbecoming the backbone of AI work\u201d\u2014all under the umbrella of \u201crenting time and storage on someone else\u2019s computers.\u201d Web3 has similarly progressed through \u201cbasic blockchain and cryptocurrency tokens\u201d to \u201cdecentralized finance\u201d to \u201cNFTs as loyalty cards.\u201d Each step has been a twist on \u201cwhat if we could write code to interact with a tamper-resistant ledger in real-time?\u201d\nMost recently, I\u2019ve been thinking about this in terms of the space we currently call \u201cAI.\u201d I\u2019ve called out the data field\u2019s rebranding efforts before; but even then, I acknowledged that these weren\u2019t just new coats of paint. Each time, the underlying implementation changed a bit while still staying true to the larger phenomenon of \u201cAnalyzing Data for Fun and Profit.\u201d\nConsider the structural evolutions of that theme:\nStage 1: Hadoop and Big Data \nBy 2008, many companies found themselves at the intersection of \u201ca steep increase in online activity\u201d and \u201ca sharp decline in costs for storage and computing.\u201d They weren\u2019t quite sure what this \u201cdata\u201d substance was, but they\u2019d convinced themselves that they had tons of it that they could monetize. All they needed was a tool that could handle the massive workload. And Hadoop rolled in.\nIn short order, it was tough to get a data job if you didn\u2019t have some Hadoop behind your name. And harder to sell a data-related product unless it spoke to Hadoop. The elephant was unstoppable.\nUntil it wasn\u2019t.\u00a0\nHadoop\u2019s value\u2014being able to crunch large datasets\u2014often paled in comparison to its costs. A basic, production-ready cluster priced out to the low-six-figures. A company then needed to train up their ops team to manage the cluster, and their analysts to express their ideas in MapReduce. Plus there was all of the infrastructure to push data into the cluster in the first place. \nIf you weren\u2019t in the terabytes-a-day club, you really had to take a step back and ask what this was all for. Doubly so as hardware improved, eating away at the lower end of Hadoop-worthy work.\nAnd then there was the other problem: for all the fanfare, Hadoop was really large-scale business intelligence (BI).\n(Enough time has passed; I think we can now be honest with ourselves. We built an entire industry by \u2026 repackaging an existing industry. This is the power of marketing.)\nDon\u2019t get me wrong. BI is useful. I\u2019ve sung its praises time and again. But the grouping and summarizing just wasn\u2019t exciting enough for the data addicts. They\u2019d grown tired of learning what is; now they wanted to know what\u2019s next.\nStage 2: Machine learning models\nHadoop could kind of do ML, thanks to third-party tools. But in its early form of a Hadoop-based ML library, Mahout still required data scientists to write in Java. And it (wisely) stuck to implementations of industry-standard algorithms. If you wanted ML beyond what Mahout provided, you had to frame your problem in MapReduce terms. Mental contortions led to code contortions led to frustration. And, often, to giving up.\n(After coauthoring Parallel R I gave a number of talks on using Hadoop. A common audience question was \u201ccan Hadoop run [my arbitrary analysis job or home-grown algorithm]?\u201d And my answer was a qualified yes: \u201cHadoop could theoretically scale your job. But only if you or someone else will take the time to implement that approach in MapReduce.\u201d That didn\u2019t go over well.)\nGoodbye, Hadoop. Hello, R and scikit-learn. A typical data job interview now skipped MapReduce in favor of white-boarding k-means clustering or random forests.\nAnd it was good. For a few years, even. But then we hit another hurdle.\nWhile data scientists were no longer handling Hadoop-sized workloads, they were trying to build predictive models on a different kind of \u201clarge\u201d dataset: so-called \u201cunstructured data.\u201d (I prefer to call that \u201csoft numbers,\u201d but that\u2019s another story.) A single document may represent thousands of features. An image? Millions.\nSimilar to the dawn of Hadoop, we were back to problems that existing tools could not solve.\nThe solution led us to the next structural evolution. And that brings our story to the present day:\nStage 3: Neural networks\nHigh-end video games required high-end video cards. And since the cards couldn\u2019t tell the difference between \u201cmatrix algebra for on-screen display\u201d and \u201cmatrix algebra for machine learning,\u201d neural networks became computationally feasible and commercially viable. It felt like, almost overnight, all of machine learning took on some kind of neural backend. Those algorithms packaged with scikit-learn? They were unceremoniously relabeled \u201cclassical machine learning.\u201d\nThere\u2019s as much Keras, TensorFlow, and Torch today as there was Hadoop back in 2010-2012. The data scientist\u2014sorry, \u201cmachine learning engineer\u201d or \u201cAI specialist\u201d\u2014job interview now involves one of those toolkits, or one of the higher-level abstractions such as HuggingFace Transformers.\nAnd just as we started to complain that the crypto miners were snapping up all of the affordable GPU cards, cloud providers stepped up to offer access on-demand. Between Google (Vertex AI and Colab) and Amazon (SageMaker), you can now get all of the GPU power your credit card can handle. Google goes a step further in offering compute instances with its specialized TPU hardware.\nNot that you\u2019ll even need GPU access all that often. A number of groups, from small research teams to tech behemoths, have used their own GPUs to train on large, interesting datasets and they give those models away for free on sites like TensorFlow Hub and Hugging Face Hub. You can download these models to use out of the box, or employ minimal compute resources to fine-tune them for your particular task.\nYou see the extreme version of this pretrained model phenomenon in the large language models (LLMs) that drive tools like Midjourney or ChatGPT. The overall idea of generative AI is to get a model to create content that could have reasonably fit into its training data. For a sufficiently large training dataset\u2014say, \u201cbillions of online images\u201d or \u201cthe entirety of Wikipedia\u201d\u2014a model can pick up on the kinds of patterns that make its outputs seem eerily lifelike.\nSince we\u2019re covered as far as compute power, tools, and even prebuilt models, what are the frictions of GPU-enabled ML? What will drive us to the next structural iteration of Analyzing Data for Fun and Profit?\nStage 4? Simulation\nGiven the progression thus far, I think the next structural evolution of Analyzing Data for Fun and Profit will involve a new appreciation for randomness. Specifically, through simulation.\nYou can see a simulation as a temporary, synthetic environment in which to test an idea. We do this all the time, when we ask \u201cwhat if?\u201d and play it out in our minds. \u201cWhat if we leave an hour earlier?\u201d\u00a0(We\u2019ll miss rush hour traffic.) \u201cWhat if I bring my duffel bag instead of the roll-aboard?\u201d (It will be easier to fit in the overhead storage.) That works just fine when there are only a few possible outcomes, across a small set of parameters.\nOnce we\u2019re able to quantify a situation, we can let a computer run \u201cwhat if?\u201d scenarios at industrial scale. Millions of tests, across as many parameters as will fit on the hardware. It\u2019ll even summarize the results if we ask nicely. That opens the door to a number of possibilities, three of which I\u2019ll highlight here:\nMoving beyond from point estimates\nLet\u2019s say an ML model tells us that this house should sell for $744,568.92. Great! We\u2019ve gotten a machine to make a prediction for us. What more could we possibly want?\nContext, for one. The model\u2019s output is just a single number, a point estimate of the most likely price. What we really want is the spread\u2014the range of likely values for that price. Does the model think the correct price falls between $743k-$746k? Or is it more like $600k-$900k? You want the former case if you\u2019re trying to buy or sell that property.\nBayesian data analysis, and other techniques that rely on simulation behind the scenes, offer additional insight here. These approaches vary some parameters, run the process a few million times, and give us a nice curve that shows how often the answer is (or, \u201cis not\u201d) close to that $744k.\nSimilarly, Monte Carlo simulations can help us spot trends and outliers in potential outcomes of a process. \u201cHere\u2019s our risk model. Let\u2019s assume these ten parameters can vary, then try the model with several million variations on those parameter sets. What can we learn about the potential outcomes?\u201d Such a simulation could reveal that, under certain specific circumstances, we get a case of total ruin. Isn\u2019t it nice to uncover that in a simulated environment, where we can map out our risk mitigation strategies with calm, level heads?\nMoving beyond point estimates is very close to present-day AI challenges. That\u2019s why it\u2019s a likely next step in Analyzing Data for Fun and Profit. In turn, that could open the door to other techniques:\nNew ways of exploring the solution space\nIf you\u2019re not familiar with evolutionary algorithms, they\u2019re a twist on the traditional Monte Carlo approach. In fact, they\u2019re like several small Monte Carlo simulations run in sequence. After each iteration, the process compares the results to its fitness function, then mixes the attributes of the top performers. Hence the term \u201cevolutionary\u201d\u2014combining the winners is akin to parents passing a mix of their attributes on to progeny. Repeat this enough times and you may just find the best set of parameters for your problem.\n(People familiar with optimization algorithms will recognize this as a twist on simulated annealing: start with random parameters and attributes, and narrow that scope over time.)\nA number of scholars have tested this shuffle-and-recombine-till-we-find-a-winner approach on timetable scheduling. Their research has applied evolutionary algorithms to groups that need efficient ways to manage finite, time-based resources such as classrooms and factory equipment. Other groups have tested evolutionary algorithms in drug discovery. Both situations benefit from a technique that optimizes the search through a large and daunting solution space.\nThe NASA ST5 antenna is another example. Its bent, twisted wire stands in stark contrast to the straight aerials with which we are familiar. There\u2019s no chance that a human would ever have come up with it.\u00a0But the evolutionary approach could, in part because it was not limited by human sense of aesthetic or any preconceived notions of what an \u201cantenna\u201d could be. It just kept shuffling the designs that satisfied its fitness function until the process finally converged.\nTaming complexity\nComplex adaptive systems are hardly a new concept, though most people got a harsh introduction at the start of the Covid-19 pandemic. Cities closed down, supply chains snarled, and people\u2014independent actors, behaving in their own best interests\u2014made it worse by hoarding supplies because they thought distribution and manufacturing would never recover. Today, reports of idle cargo ships and overloaded seaside ports remind us that we shifted from under- to over-supply. The mess is far from over.\nWhat makes a complex system troublesome isn\u2019t the sheer number of connections. It\u2019s not even that many of those connections are invisible because a person can\u2019t see the entire system at once. The problem is that those hidden connections only become visible during a malfunction: a failure in Component B affects not only neighboring Components A and C, but also triggers disruptions in T and R. R\u2019s issue is small on its own, but it has just led to an outsized impact in \u03a6 and \u03a3. \n(And if you just asked \u201cwait, how did Greek letters get mixed up in this?\u201d then \u2026\u00a0 you get the point.)\nOur current crop of AI tools is powerful, yet ill-equipped to provide insight into complex systems. We can\u2019t surface these hidden connections using a collection of independently-derived point estimates; we need something that can simulate the entangled system of independent actors moving all at once.\nThis is where agent-based modeling (ABM) comes into play. This technique simulates interactions in a complex system. Similar to the way a Monte Carlo simulation can surface outliers, an ABM can catch unexpected or unfavorable interactions in a safe, synthetic environment.\nFinancial markets and other economic situations are prime candidates for ABM. These are spaces where a large number of actors behave according to their rational self-interest, and their actions feed into the system and affect others\u2019 behavior. According to practitioners of complexity economics (a study that owes its origins to the Sante Fe Institute), traditional economic modeling treats these systems as though they run in an equilibrium state and therefore fails to identify certain kinds of disruptions. ABM captures a more realistic picture because it simulates a system that feeds back into itself.\nSmoothing the on-ramp\nInterestingly enough, I haven\u2019t mentioned anything new or ground-breaking. Bayesian data analysis and Monte Carlo simulations are common in finance and insurance. I was first introduced to evolutionary algorithms and agent-based modeling more than fifteen years ago. (If memory serves, this was shortly before I shifted my career to what we now call AI.) And even then I was late to the party.\nSo why hasn\u2019t this next phase of Analyzing Data for Fun and Profit taken off?\nFor one, this structural evolution needs a name. Something to distinguish it from \u201cAI.\u201d Something to market. I\u2019ve been using the term \u201csynthetics,\u201d so I\u2019ll offer that up. (Bonus: this umbrella term neatly includes generative AI\u2019s ability to create text, images, and other realistic-yet-heretofore-unseen data points. So we can ride that wave of publicity.)\nNext up is compute power. Simulations are CPU-heavy, and sometimes memory-bound. Cloud computing providers make that easier to handle, though, so long as you don\u2019t mind the credit card bill. Eventually we\u2019ll get simulation-specific hardware\u2014what will be the GPU or TPU of simulation?\u2014but I think synthetics can gain traction on existing gear.\nThe third and largest hurdle is the lack of simulation-specific frameworks. As we surface more use cases\u2014as we apply these techniques to real business problems or even academic challenges\u2014we\u2019ll improve the tools because we\u2019ll want to make that work easier. As the tools improve, that reduces the costs of trying the techniques on other use cases. This kicks off another iteration of the value loop. Use cases tend to magically appear as techniques get easier to use.\nIf you think I\u2019m overstating the power of tools to spread an idea, imagine trying to solve a problem with a new toolset while also creating that toolset at the same time. It\u2019s tough to balance those competing concerns. If someone else offers to build the tool while you use it and road-test it, you\u2019re probably going to accept. This is why these days we use TensorFlow or Torch instead of hand-writing our backpropagation loops.\nToday\u2019s landscape of simulation tooling is uneven. People doing Bayesian data analysis have their choice of two robust, authoritative offerings in Stan and PyMC3, plus a variety of books to understand the mechanics of the process. Things fall off after that. Most of the Monte Carlo simulations I\u2019ve seen are of the hand-rolled variety. And a quick survey of agent-based modeling and evolutionary algorithms turns up a mix of proprietary apps and nascent open-source projects, some of which are geared for a particular problem domain.\nAs we develop the authoritative toolkits for simulations\u2014the TensorFlow of agent-based modeling and the Hadoop of evolutionary algorithms, if you will\u2014expect adoption to grow. Doubly so, as commercial entities build services around those toolkits and rev up their own marketing (and publishing, and certification) machines. \nTime will tell \nMy expectations of what to come are, admittedly, shaped by my experience and clouded by my interests. Time will tell whether any of this hits the mark.\nA change in business or consumer appetite could also send the field down a different road. The next hot device, app, or service will get an outsized vote in what companies and consumers expect of technology.\nStill, I see value in looking for this field\u2019s structural evolutions. The wider story arc changes with each iteration to address changes in appetite. Practitioners and entrepreneurs, take note.\nJob-seekers should do the same. Remember that you once needed Hadoop on your r\u00e9sum\u00e9 to merit a second look; nowadays it\u2019s a liability.\u00a0Building models is a desired skill for now, but it\u2019s slowly giving way to robots.\u00a0So do you really think it\u2019s too late to join the data field? I think not.\nKeep an eye out for that next wave. That\u2019ll be your time to jump in.",
  "link": "https://www.oreilly.com/radar/structural-evolutions-in-data/"
 },
 {
  "title": "The Real Problem with Software Development",
  "content": "A few weeks ago, I saw a tweet that said \u201cWriting code isn\u2019t the problem. Controlling complexity is.\u201d I wish I could remember who said that; I will be quoting it a lot in the future. That statement nicely summarizes what makes software development difficult. It\u2019s not just memorizing the syntactic details of some programming language, or the many functions in some API, but understanding and managing the complexity of the problem you\u2019re trying to solve.\nWe\u2019ve all seen this many times.\u00a0Lots of applications and tools start simple. They do 80% of the job well, maybe 90%. But that isn\u2019t quite enough. Version 1.1 gets a few more features, more creep into version 1.2, and by the time you get to 3.0, an elegant user interface has turned into a mess. This increase in complexity is one reason that applications tend to become less useable over time. We also see this phenomenon as one application replaces another. RCS was useful, but didn\u2019t do everything we needed it to; SVN was better; Git does just about everything you could want, but at an enormous cost in complexity. (Could Git\u2019s complexity be managed better? I\u2019m not the one to say.) OS X, which used to trumpet \u201cIt just works,\u201d has evolved to \u201cit used to just work\u201d; the most user-centric Unix-like system ever built now staggers under the load of new and poorly thought-out features.\nThe problem of complexity isn\u2019t limited to user interfaces; that may be the least important (though most visible) aspect of the problem. Anyone who works in programming has seen the source code for some project evolve from something short, sweet, and clean to a seething mass of bits. (These days, it\u2019s often a seething mass of distributed bits.) Some of that evolution is driven by an increasingly complex world that requires attention to secure programming, cloud deployment, and other issues that didn\u2019t exist a few decades ago. But even here: a requirement like security tends to make code more complex\u2014but complexity itself hides security issues. Saying \u201cyes, adding security made the code more complex\u201d is wrong on several fronts. Security that\u2019s added as an afterthought almost always fails. Designing security in from the start almost always leads to a simpler result than bolting security on as an afterthought, and the complexity will stay manageable if new features and security grow together. If we\u2019re serious about complexity, the complexity of building secure systems needs to be managed and controlled in step with the rest of the software, otherwise it\u2019s going to add more vulnerabilities.\nThat brings me to my main point. We\u2019re seeing more code that\u2019s written (at least in first draft) by generative AI tools, such as GitHub Copilot, ChatGPT (especially with Code Interpreter), and Google Codey. One advantage of computers, of course, is that they don\u2019t care about complexity. But that advantage is also a significant disadvantage. Until AI systems can generate code as reliably as our current generation of compilers, humans will need to understand\u2014and debug\u2014the code they write. Brian Kernighan wrote that \u201cEveryone knows that debugging is twice as hard as writing a program in the first place. So if you\u2019re as clever as you can be when you write it, how will you ever debug it?\u201d We don\u2019t want a future that consists of code too clever to be debugged by humans\u2014at least not until the AIs are ready to do that debugging for us. Really brilliant programmers write code that finds a way out of the complexity: code that may be a little longer, a little clearer, a little less clever so that someone can understand it later. (Copilot running in VSCode has a button that simplifies code, but its capabilities are limited.)\nFurthermore, when we\u2019re considering complexity, we\u2019re not just talking about individual lines of code and individual functions or methods. Most professional programmers work on large systems that can consist of thousands of functions and millions of lines of code. That code may take the form of dozens of microservices running as asynchronous processes and communicating over a network. What is the overall structure, the overall architecture, of these programs? How are they kept simple and manageable? How do you think about complexity when writing or maintaining software that may outlive its developers? Millions of lines of legacy code going back as far as the 1960s and 1970s are still in use, much of it written in languages that are no longer popular. How do we control complexity when working with these?\nHumans don\u2019t manage this kind of complexity well, but that doesn\u2019t mean we can check out and forget about it. Over the years, we\u2019ve gradually gotten better at managing complexity. Software architecture is a distinct specialty that has only become more important over time. It\u2019s growing more important as systems grow larger and more complex, as we rely on them to automate more tasks, and as those systems need to scale to dimensions that were almost unimaginable a few decades ago. Reducing the complexity of modern software systems is a problem that humans can solve\u2014and I haven\u2019t yet seen evidence that generative AI can. Strictly speaking, that\u2019s not a question that can even be asked yet. Claude 2 has a maximum context\u2014the upper limit on the amount of text it can consider at one time\u2014of 100,000 tokens1; at this time, all other large language models are significantly smaller. While 100,000 tokens is huge, it\u2019s much smaller than the source code for even a moderately sized piece of enterprise software. And while you don\u2019t have to understand every line of code to do a high-level design for a software system, you do have to manage a lot of information: specifications, user stories, protocols, constraints, legacies and much more. Is a language model up to that?\nCould we even describe the goal of \u201cmanaging complexity\u201d in a prompt? A few years ago, many developers thought that minimizing \u201clines of code\u201d was the key to simplification\u2014and it would be easy to tell ChatGPT to solve a problem in as few lines of code as possible.\u00a0But that\u2019s not really how the world works, not now, and not back in 2007. Minimizing lines of code sometimes leads to simplicity, but just as often leads to complex incantations that pack multiple ideas onto the same line, often relying on undocumented side effects. That\u2019s not how to manage complexity. Mantras like DRY (Don\u2019t Repeat Yourself) are often useful (as is most of the advice in The Pragmatic Programmer), but I\u2019ve made the mistake of writing code that was overly complex to eliminate one of two very similar functions. Less repetition, but the result was more complex and harder to understand. Lines of code are easy to count, but if that\u2019s your only metric, you will lose track of qualities like readability that may be more important. Any engineer knows that design is all about tradeoffs\u2014in this case, trading off repetition against complexity\u2014but difficult as these tradeoffs may be for humans, it isn\u2019t clear to me that generative AI can make them any better, if at all.\nI\u2019m not arguing that generative AI doesn\u2019t have a role in software development. It certainly does. Tools that can write code are certainly useful: they save us looking up the details of library functions in reference manuals, they save us from remembering the syntactic details of the less commonly used abstractions in our favorite programming languages. As long as we don\u2019t let our own mental muscles decay, we\u2019ll be ahead. I am arguing that we can\u2019t get so tied up in automatic code generation that we forget about controlling complexity. Large language models don\u2019t help with that now, though they might in the future. If they free us to spend more time understanding and solving the higher-level problems of complexity, though, that will be a significant gain.\nWill the day come when a large language model will be able to write a million line enterprise program? Probably. But someone will have to write the prompt telling it what to do. And that person will be faced with the problem that has characterized programming from the start: understanding complexity, knowing where it\u2019s unavoidable, and controlling it.\n\nFootnotes\nIt\u2019s common to say that a token is approximately \u2158 of a word. It\u2019s not clear how that applies to source code, though. It\u2019s also common to say that 100,000 words is the size of a novel, but that\u2019s only true for rather short novels.",
  "link": "https://www.oreilly.com/radar/the-real-problem-with-software-development/"
 },
 {
  "title": "Radar Trends to Watch: September 2023",
  "content": "While the AI group is still the largest, it\u2019s notable that Programming, Web, and Security are all larger than they\u2019ve been in recent months. One reason is certainly that we\u2019re pushing AI news into other categories as appropriate. But I also think that it\u2019s harder to impress with AI than it used to be. AI discussions have been much more about regulation and intellectual property\u2014which makes me wonder whether legislation should be a separate category.\nThat notwithstanding, it\u2019s important that OpenAI is now allowing API users to fine-tune their GPT-4 apps. It\u2019s as-a-service, of course. And RISC-V finally appears to be getting some serious adoption. Could it compete with Atom and Intel? We shall see.\nAI\nOpenAI has announced ChatGPT Enterprise, a version of ChatGPT that targets enterprise customers. ChatGPT Enterprise offers improved security, a promise that they won\u2019t train on your conversations, single sign on, an admin console, a larger 32K context, higher performance, and the elimination of usage caps.Facebook/Meta has released Code LLaMA, a version of their LLaMA 2 model that has been specialized for writing code. It can be used for code generation or completion. Its context window is 100,000 tokens, allowing Code LLaMA to be more accurate on larger programs.OpenAI has announced that API users can now fine-tune GPT-3.5 for their own applications.\u00a0Fine-tuning for GPT-4 will come later. To preserve safety, tuning data is passed through OpenAI\u2019s moderation filter.txtai is an open source embeddings database. It is a vector database that has been designed specifically to work with natural language problems.TextFX is a set of tools that use Google\u2019s PaLM 2 model to play with language. It doesn\u2019t answer questions or write poems; it allows users to see the possibilities in words as an aid to their own creativity.A US judge has ruled that an AI system cannot copyright a work. In this case, the AI itself\u2014not the human user\u2014was to hold the copyright. This ruling is in line with the Copyright Office\u2019s guidance: giving prompts to a generative algorithm isn\u2019t sufficient to create a copyrightable work.Despite an error rate of roughly 50% for ChatGPT, a study shows that users prefer ChatGPT\u2019s answers to programming questions over answers from StackOverflow. ChatGPT\u2019s complete, articulate, and polite answers appear to be the cause of this preference.AI was on the agenda at DefCon and, while results of a red teaming competition won\u2019t be released for some months, it\u2019s clear that security remains an afterthought, and that attacking the current AI models is extremely easy.Emotion recognition is difficult, if not impossible. It is not clear that there are any credible use cases for it. AI systems are particularly bad at it.\u00a0But companies are building products.Watermarking has been proposed as a technique for identifying whether content was generated by AI, but it\u2019s not a panacea. Here are some questions to help evaluate whether watermarks are useful in any given situation.Zoom and Grammarly have both issued new license agreements that allow them to use data collected from users to train AI. Zoom has backed down after customer backlash, but that begs the question: Will other applications follow?Using large language models for work or play is one thing, but how do you put one into production? 7 Frameworks for Serving LLMs surveys some tools for deploying language models.Simon Willison provides instructions for running LLaMA 2 on a Mac. He also provides slides and a well-edited transcript of his talk about LLMs at North Bay Python.PhotoGuard is a tool for protecting photos and other images from manipulation by AI systems. It adds data to the image in ways that aren\u2019t detectable by humans, but that introduce noticeable distortions when the image is modified.C2PA is a cryptographic protocol for attesting to the provenance of electronic documents. It could be used for specifying whether documents are generated by AI.Google\u2019s DeepMind has built a vision-language-action model called RT-2 (Robotic Transformer 2) that combines vision and language with the ability to control a robot. It learns both from web data (images and text) and robotic data (interactions with physical objects).\nProgramming\nMaccarone is an extension to VSCode that allows you to \u201cdelegate\u201d blocks of Python code to AI (GPT-4). The portions of the code that are under AI control are automatically updated as needed when the surrounding code is changed.Microsoft is adding Python as a scripting language for Excel formulas. Python code executes in an Azure container that includes some commonly used libraries, including Matplotlib and Pandas.Many companies are building platform engineering teams as a means of making software developers more effective. Here are some ideas about getting started with platform engineering.A Google study of its in-house Rust use supports the claim that Rust makes it easier to produce high-quality code. The study also busts a number of myths about the language.\u00a0It isn\u2019t as hard to learn as most people think (then again, this is a Google study).deno_python is a Javascript module that allows integration between Javascript (running on Deno) and Python, allowing Javascript programmers to call important Python libraries and call Python functions.The Python Steering Council has announced that it will make the Global Interpreter Lock (GIL) optional in a future version of Python. Python\u2019s GIL has long been a barrier to effective multi-threaded computing. The change will be backwards-compatible.\nWeb\nGoogle\u2019s controversial Web Environment Integrity proposal provides a way for web servers to cryptographically authenticate the browser software making a request. WEI could potentially reduce online fraud, but it also presents some significant privacy risks.Trafilatura is a new tool for web scraping that has been designed with quantitative research (for example, assembling training data for language models). It can extract text and metadata from HTML, and generate output in a number of formats.Astro is yet another open source web framework that\u2019s designed for high performance and ease of development.While the \u201cbrowser wars\u201d are far behind us, it is still difficult for developers to write code that works correctly on all browsers. Baseline is a project of the W3C\u2019s WebDX Community Group that specifies which features web developers can rely on in the most widely used browsers. How Large Language Models Assisted a Website Makeover raises some important questions: When do you stop using ChatGPT and finish the job yourself?\u00a0 When does your own ability start to atrophy?Remember Flash? It has a museum\u2026 And Flash games will run in a modern browser using Ruffle, a Flash Player emulator that is written in WebAssembly.\nSecurity\nProof-of-work makes it to the Tor network. It is used as a defense against denial of service attacks. PoW is disabled most of the time, but when traffic seems unusually high, it can switch on, forcing users to \u201cprove\u201d their humanness (actually, their willingness to perform work).A retrospective on this year\u2019s MoveIT attack draws some important conclusions about protecting your assets. Mapping the supply chain, third party risk management, zero trust, and continuous penetration testing are all important parts of a security plan.Bitwarden has released an open source end-to-end encrypted secrets manager. The secrets manager allows safe distribution of API keys, certificates and other sensitive data.The US Government has announced the AI Cybersecurity Challenge (AIxCC). AIxCC is a two year competition to build AI systems that can secure critical software. There\u2019s $18.5 million in prizes, plus the possibility of DARPA funding for up to seven companies.OSC&R is the Open Source Supply Chain Attack Reference, a new project that catalogs and describes techniques used to attack software supply chains. It is modeled on MITRE\u2019s ATT&CK framework.The Lapsus$ group has become one of the most effective threat actors, despite being relatively unsophisticated. They rely on persistence, clever social engineering, and analyzing weak points in an organization\u2019s security posture rather than compromising infrastructure.The NSA has issued a report that gives guidance on how to protect systems against memory safety bugs.Bruce Schneier has an important take on the long-term consequences of the SolarWinds attack. Those consequences include the theft of an Azure customer account signing key that in turn has been used by attackers to access US government email accounts.A new generation of ransomware attacks is targeting IT professionals via fake advertisements for IT tools. While IT professionals are (presumably) more wary and aware than other users, they are also high-value targets.\nHardware\nParmesan cheese producers are experimenting with adding microchips to the cheese rind to authenticate genuine cheese.Adoption of RISC-V, a royalty-free open source instruction set architecture for microprocessors, has been increasing. Could it displace ARM?Speculative execution bugs have been discovered for recent Intel (\u201cDownfall\u201d) and AMD (\u201cInception\u201d) processors. Patches for Linux have been released.\nOperations\nSince Hashicorp has moved Terraform from the open source Mozilla Public License to the Business Source License, the OpenTF foundation has been created, and has forked the Terraform project to create OpenTF.There\u2019s a rise in the abuse of Cloudflare tunnels to create persistent malicious communications channels.Amazon has announced that they will begin charging for public IPv4 addresses. There are already some charges for Elastic IP addresses. Users won\u2019t be charged for IP addresses they already own. Among other things, this change is intended to accelerate IPv6 adoption.\nQuantum Computing\nPeter Shor, inventor of the quantum algorithm for factoring prime numbers (which in turn could be used to break most modern cryptography that isn\u2019t quantum-resistant), has published the lecture notes from the course on quantum computing that he teaches at MIT.A Honeywell quantum computer has been used to find a material that can improve solar cell efficiency. It\u2019s likely that the first applications of quantum computing will involve simulating quantum phenomena rather than pure computation.\nCryptocurrency\nIf you\u2019re interested in iris-scanning WorldCoin, a cryptographer analyzes the privacy promises made by their system. He remains skeptical, but came away less unimpressed than he expected to be.Paypal has introduced a stablecoin that claims to be fully backed by US dollars.\nBiology\nLabGenius is a company that combines synthetic biology, artificial intelligence, and robotics to design and build new human antibodies that are effective against hard-to-treat diseases.",
  "link": "https://www.oreilly.com/radar/radar-trends-to-watch-september-2023/"
 },
 {
  "title": "The next generation of developer productivity",
  "content": "To follow up on our previous survey about low-code and no-code tools, we decided to run another short survey about tools specifically for software developers\u2014including, but not limited to, GitHub Copilot and ChatGPT. We\u2019re interested in how \u201cdeveloper enablement\u201d tools of all sorts are changing the workplace. Our survey 1 showed that while these tools increased productivity, they aren\u2019t without their costs. Both upskilling and retraining developers to use these tools are issues.\nFew professional software developers will find it surprising that software development teams are respondents said that productivity is the biggest challenge their organization faced, and another 19% said that time to market and deployment speed are the biggest challenges. Those two answers are almost the same: decreasing time to market requires increasing productivity, and improving deployment speed is itself an increase in productivity. Together, those two answers represented 48% of the respondents, just short of half.\nHR issues were the second-most-important challenge, but they\u2019re nowhere near as pressing. 12% of the respondents reported that job satisfaction is the greatest challenge; 11% said that there aren\u2019t good job candidates to hire; and 10% said that employee retention is the biggest issue. Those three challenges total 33%, just one-third of the respondents.\n 1 Our survey ran from April 18 to April 25, 2023. There were 739 responses. \nIt\u2019s heartening to realize that hiring and retention are still challenges in this time of massive layoffs, but it\u2019s also important to realize that these issues are less important than productivity.\nBut the big issue, the issue we wanted to explore, isn\u2019t the challenges themselves; it\u2019s what organizations are doing to meet them. A surprisingly large percentage of respondents (28%) aren\u2019t making any changes to become more productive. But 20% are changing their onboarding and upskilling processes, 15% are hiring new developers, and 13% are using self-service engineering platforms.\nWe found that the biggest struggle for developers working with new tools is training (34%), and another 12% said the biggest struggle is \u201cease of use.\u201d Together, that\u2019s almost half of all respondents (46%). That was a surprise, since many of these tools are supposed to be low- or no-code. We\u2019re thinking specifically about tools like GitHub Copilot, Amazon CodeWhisperer, and other code generators, but almost all productivity tools claim to make life simpler. At least at first, that\u2019s clearly not true. There\u2019s a learning curve, and it appears to be steeper than we\u2019d have guessed. It\u2019s also worth noting that 13% of the respondents said that the tools \u201cdidn\u2019t effectively solve the problems that developers face.\u201d\nOver half of the respondents (51%) said that their organizations are using self-service deployment pipelines to increase productivity. Another 13% said that while they\u2019re using self-service pipelines, they haven\u2019t seen an increase in productivity. So almost two-thirds of the respondents are using self-service pipelines for deployment, and for most of them, the pipelines are working\u2014reducing the overhead required to put new projects into production.\nFinally, we wanted to know specifically about the effect of GitHub Copilot, ChatGPT, and other AI-based programming tools. Two-thirds of the respondents (67%) reported that these tools aren\u2019t in use at their organizations. We suspect this estimate is lowballing Copilot\u2019s actual usage. Back in the early 2000s, a widely quoted survey reported that CIOs almost unanimously said that their IT organizations weren\u2019t making use of open source. How little they knew! Actual usage of Copilot, ChatGPT, and similar tools is likely to be much higher than 33%. We\u2019re sure that even if they aren\u2019t using Copilot or ChatGPT on the job, many programmers are experimenting with these tools or using them on personal projects.\nWhat about the 33% who reported that Copilot and ChatGPT are in use at their organizations? First, realize that these are early adopters: Copilot was only released a year and a half ago, and ChatGPT has been out for less than a year. It\u2019s certainly significant that they (and similar tools) have grabbed a third of the market in that short a period. It\u2019s also significant that making a commitment to a new way of programming\u2014and these tools are nothing if not a new kind of programming\u2014is a much bigger change than, say, signing up for a ChatGPT account.\n11% of the respondents said their organizations use Copilot and ChatGPT, and that the tools are primarily useful to junior developers; 13% said they\u2019re primarily useful to senior developers. Another 9% said that the tools haven\u2019t yielded an increase in productivity. The difference between junior and senior developers is closer than we expected. Common wisdom is that Copilot is more of an advantage to senior programmers, who are better able to describe the problem they need to solve in an intricate set of prompts and to notice bugs in the generated code quickly. Our survey hints that the difference between senior and junior developers is relatively small\u2014although they\u2019re almost certainly using Copilot in different ways. Junior developers are using it to learn and to spend less time solving problems by looking up solutions on Stack Overflow or searching online documentation. Senior developers are using it to help design and structure systems, and even to create production code.\nIs developer productivity an issue? Of course; it always is. Part of the solution is improved tooling: self-service deployment, code-generation tools, and other new technologies and ideas. Productivity tools\u2014and specifically the successors to tools like Copilot\u2014are remaking software development in radical ways. Software developers are getting value from these tools, but don\u2019t let the buzz fool you: that value doesn\u2019t come for free. Nobody\u2019s going to sit down with ChatGPT, type \u201cGenerate an enterprise application for selling shoes,\u201d and come away with something worthwhile. Each has its own learning curve, and it\u2019s easy to underestimate how steep that curve can be. Developer productivity tools will be a big part of the future; but to take full advantage of those tools, organizations will need to plan for skills development.",
  "link": "https://www.oreilly.com/radar/the-next-generation-of-developer-productivity/"
 },
 {
  "title": "The ChatGPT Surge",
  "content": "I\u2019m sure that nobody will be surprised that the number of searches for ChatGPT on the O\u2019Reilly learning platform skyrocketed after its release in November, 2022. It might be a surprise how quickly it got to the top of our charts: it peaked in May as the 6th most common search query. Then it dropped almost as quickly: it dropped back to #8 in June, and fell further to #19 in July. At its peak, ChatGPT was in very exclusive company: it\u2019s not quite on the level of Python, Kubernetes, and Java, but it\u2019s in the mix with AWS and React, and significantly ahead of Docker.\nA look at the number of searches for terms commonly associated with AI shows how dramatic this rise was:\n\nChatGPT came from nowhere to top all the AI search terms except for Machine Learning itself, which is consistently our #3 search term\u2014and, despite ChatGPT\u2019s dramatic decline in June and July, it\u2019s still ahead of all other search terms relevant to AI. The number of searches for Machine Learning itself held steady, though it arguably declined slightly when ChatGPT appeared. What\u2019s more interesting, though, is that the search term \u201cGenerative AI\u201d suddenly emerged from the pack as the third most popular search term. If current trends continue, in August we might see more searches for Generative AI than for ChatGPT.\nWhat can we make of this? Everyone knows that ChatGPT had one of the most successful launches of any software project, passing a million users in its first five days. (Since then, it\u2019s been beaten by Facebook\u2019s Threads, though that\u2019s not really a fair comparison.) There are plenty of reasons for this surge. Talking computers have been a science fiction dream since well before Star Trek\u2014by itself, that\u2019s a good reason for the public\u2019s fascination. ChatGPT might simplify common tasks, from doing research to writing essays to basic programming, so many people want to use it to save labor\u2014though getting it to do quality work is more difficult than it seems at first glance. (We\u2019ll leave the issue of whether this is \u201ccheating\u201d to the users, their teachers, and their employers.) And, while I\u2019ve written frequently about how ChatGPT will change programming, it will undoubtedly have an even greater effect on non-programmers. It will give them the chance to tell computers what to do without programming; it\u2019s the ultimate \u201clow code\u201d experience.\nSo there are plenty of reasons for ChatGPT to surge. What about other search terms? It\u2019s easy to dismiss these search queries as also-rans, but they were all in the top 300 for May, 2023\u2014and we typically have a few million unique search terms per month. Removing ChatGPT and Machine Learning from the previous graph makes it easier to see trends in the other popular search terms:\n\nIt\u2019s mostly \u201cup and to the right.\u201d Three search terms stand out: Generative AI, LLM, and Langchain all follow similar curves: they start off with relatively moderate growth that suddenly becomes much steeper in February, 2023. We\u2019ve already noted that the number of searches for Generative AI increased sharply since the release of ChatGPT, and haven\u2019t declined in the past two months. Our users evidently prefer LLM to spelling out \u201cLarge Language Models,\u201d but if you add these two search terms together, the total number of searches for July is within 1% of Generative AI. This surge didn\u2019t really start until last November, when it was spurred by the appearance of ChatGPT\u2014even though search terms like LLM were already in circulation because of GPT-3, DALL-E, StableDiffusion, Midjourney, and other language-based generative AI tools.\nUnlike LLMs, Langchain didn\u2019t exist prior to ChatGPT\u2014but once it appeared, the number of searches took off rapidly, and didn\u2019t decline in June and July.\u00a0That makes sense; although it\u2019s still early, Langchain looks like it will be the cornerstone of LLM-based software development. It\u2019s a widely used platform for building applications that generate queries programmatically and that connects LLMs with each other, with databases, and with other software. Langchain is frequently used to look up relevant articles that weren\u2019t in ChatGPT\u2019s training data and package them as part of a lengthy prompt.\nIn this group, the only search term that seems to be in a decline is Natural Language Processing. Although large language models clearly fall into the category of NLP, we suspect that most users associate NLP with older approaches to building chatbots. Searches for Artificial Intelligence appear to be holding their own, though it\u2019s surprising that there are so few searches for AI compared to Machine Learning. The difference stems from O\u2019Reilly\u2019s audience, which is relatively technical and prefers the more precise term Machine Learning. Nevertheless, the number of searches for AI rose with the release of ChatGPT, possibly because ChatGPT\u2019s appeal wasn\u2019t limited to the technical community.\nNow that we\u2019ve run through the data, we\u2019re left with the big question: What happened to ChatGPT? Why did it decline from roughly 5,000 searches to slightly over 2,500 in a period of two months? There are many possible reasons. Perhaps students stopped using ChatGPT for homework assignments as graduation and summer vacation approached. Perhaps ChatGPT has saturated the world; people know what they need to know, and are waiting for the next blockbuster. An article in Ars Technica notes that ChatGPT usage declined from May to June, and suggests many possible causes, including attention to the Twitter/Threads drama and frustration because OpenAI implemented stricter guardrails to prevent abuse. It would be unfortunate if ChatGPT usage is declining because people can\u2019t use it to generate abusive content, but that\u2019s a different article\u2026\nA more important reason for this decline might be that ChatGPT is no longer the only game in town. There are now many alternative language models. Most of these alternatives descend from Meta\u2019s LLaMA and Georgi Gerganov\u2019s llama.cpp (which can run on laptops, cell phones, and even Raspberry Pi). Users can train these models to do whatever they want. Some of these models already have chat interfaces, and all of them could support chat interfaces with some fairly simple programming. None of these alternatives generate significant search traffic at O\u2019Reilly, but that doesn\u2019t mean that they won\u2019t in the future, or that they aren\u2019t an important part of the ecosystem. Their proliferation is an important piece of evidence about what\u2019s happening among O\u2019Reilly\u2019s users. AI developers now need to ask a question that didn\u2019t even exist last November: should they build on large foundation models like ChatGPT or Google\u2019s Bard, using public APIs and paying by the token? Or should they start with an open source model that can run locally and be trained for their specific application?\nThis last explanation makes a lot of sense in context. We\u2019ve moved beyond the initial phase, when ChatGPT was a fascinating toy.\u00a0We\u2019re now building applications and incorporating language models into products, so trends in search terms have shifted accordingly. A developer interested in building with large language models needs more context; learning about ChatGPT by itself isn\u2019t enough. Developers who want to learn about language models need different kinds of information, information that\u2019s both deeper and broader. They need to learn about how generative AI works, about new LLMs, about programming with Langchain and other platforms. All of these search terms increased while ChatGPT declined. Now that there are options, and now that everyone has had a chance to try out ChatGPT, the first step in an AI project isn\u2019t to search for ChatGPT. It\u2019s to get a sense of the landscape, to discover the possibilities.\nSearches for ChatGPT peaked quickly, and are now declining rapidly\u2014and who knows what August and September will bring? (We wouldn\u2019t be surprised to see ChatGPT bounce back as students return to school and homework assignments.) The real news is that ChatGPT is no longer the whole story: you can\u2019t look at the decline in ChatGPT without also considering what else our users are searching for as they start building AI into other projects. Large language models are very clearly part of the future. They will change the way we work and live, and we\u2019re just at the start of the revolution.",
  "link": "https://www.oreilly.com/radar/the-chatgpt-surge/"
 },
 {
  "title": "Radar Trends to Watch: August 2023",
  "content": "Artificial Intelligence continues to dominate the news. In the past month, we\u2019ve seen a number of major updates to language models: Claude 2, with its 100,000 token context limit; LLaMA 2, with (relatively) liberal restrictions on use; and Stable Diffusion XL, a significantly more capable version of Stable Diffusion. Does Claude 2\u2019s huge context really change what the model can do? And what role will open access and open source language models have as commercial applications develop?\nArtificial Intelligence\nStable Diffusion XL is a new generative model that expands on the abilities of Stable Diffusion. It promises shorter, easier prompts; the ability to generate text within images correctly; the ability to be trained on private data; and of course, higher quality output. Try it on clipdrop.OpenAI has withdrawn OpenAI Classifier, a tool that was supposed to detect AI-generated text, because it was not accurate enough.ChatGPT has added a new feature called \u201cCustom Instructions.\u201d\u00a0 This feature lets users specify an initial prompt that ChatGPT processes prior to any other user-generated prompts; essentially, it\u2019s a personal \u201csystem prompt.\u201d Something to make prompt injection more fun.Qualcomm is working with Facebook/Meta to run LLaMA 2 on small devices like phones, enabling AI applications to run locally. The distinction between open source and other licenses will prove much less important than the size of the machine on which the target runs.StabilityAI has released two new large language models, FreeWilly1 and FreeWilly2. They are based on LLaMA and LLaMA 2 respectively. They are called Open Access (as opposed to Open Source), and claim performance similar to GPT 3.5 for some tasks.Chatbot Arena lets chatbots do battle with each other. Users enter prompts, which are sent to two unnamed (randomly chosen?) language models. After the responses have been generated, users can declare a winner, and find out which models have been competing.GPT-4\u2019s ability to generate correct answers to problems may have degraded over the past few months\u2014in particular, its ability to solve mathematical problems and generate correct Python code seems to have suffered. On the other hand, it is more robust against jailbreaking attacks.Facebook/Meta has released Llama 2. While there are fewer restrictions on its use than other models, it is not open source despite Facebook\u2019s claims.Autochain is a lightweight, simpler alternative to Langchain. It allows developers to build complex applications on top of large language models and databases.Elon Musk has announced his new AI company, xAI. Whether this will actually contribute to AI or be another sideshow is anyone\u2019s guess.Anthropic has announced Claude 2, a new version of their large language model. A chat interface is available at claude.ai, and API access is available. Claude 2 allows prompts of up to 100,000 tokens, much larger than other LLMs, and can generate output up to \u201ca few thousand tokens\u201d in length.parsel is a framework that helps large language models do a better job on tasks involving hierarchical multi-step reasoning and problem solving.gpt-prompt-engineer is a tool that reads a description of the task you want an AI to perform, plus a number of test cases. It then generates a large number of prompts about a topic, tests the prompts, and rates the results.LlamaIndex is a data framework (sometimes called an \u201corchestration framework\u201d) for language models that simplifies the process of indexing a user\u2019s data and using that data to build complex prompts for language models. It can be used with Langchain to build complex AI applications.OpenAI is gradually releasing its Code Interpreter, which will allow ChatGPT to execute any code that it creates, using data provided by the user, and sending output back to the user. Code interpreter reduces hallucinations, errors, and bad math.Humans can now beat AI at Go by finding and exploiting weaknesses in the AI system\u2019s play, tricking the AI into making serious mistakes.Time for existential questions: Does a single banana exist? Midjourney doesn\u2019t think so. Seriously, this is an excellent article about the difficulty of designing prompts that deliver appropriate results.The Jolly Roger Telephone Company has developed GPT\u20134-based voicebots that you can hire to answer your phone when telemarketers call. If you want to listen in, the results can be hilarious.Apache Spark now has an English SDK. It goes a step beyond tools like CoPilot, allowing you to use English directly when writing code.Humans may be more likely to believe misinformation generated by AI, possibly because AI-generated text is better structured than most human text. Or maybe because AIs are very good at being convincing.OpenOrca is yet another LLaMA-based open source language model and dataset. Its goal is to reproduce the training data for Microsoft\u2019s Orca, which was trained using chain-of-thought prompts and responses from GPT-4. The claim for both Orca models is that it can reproduce GPT-4\u2019s \u201creasoning\u201d processes.At its developer summit, Snowflake announced Document AI: natural language queries of collections of unstructured documents. This product is based on their own large language model, not an AI provider.\nProgramming\n\u201cIt works on my machine\u201d has become \u201cIt works in my container\u201d: This article has some good suggestions about how to avoid a problem that has plagued computer users for decades.StackOverflow is integrating AI into its products. StackOverflow for Teams now has a chatbot to help solve technical problems, along with a new GenAI StackExchange for discussing generative AI, prompt writing, and related issues.It isn\u2019t news that GitHub can leak private keys and authentication secrets. But a study of the containers available on DockerHub shows that Docker containers also leak keys and secrets, and many of these keys are in active use.Firejail is a Linux tool that can run any process in a private, secure sandbox.Complex and complicated: what\u2019s the difference? It has to do with information, and it\u2019s important to understand in an era of \u201ccomplex systems.\u201d First in a series.npm-manifest-check is a tool that checks the contents of a package in NPM against the package\u2019s manifest. It is a partial solution to the problem of malicious packages in NPM.Facebook has described their software development platform, much of which they have open sourced. Few developers have to work with software projects this large, but their tools (which include testing frameworks, version control, and a build system) are worth investigating.Polyrhythmix is a command-line program for generating polyrhythmic drum parts. No AI involved.Philip Guo\u2019s \u201cReal-Real-World Programming with ChatGPT\u201d shows what it\u2019s like to use ChatGPT to do a real programming task: what works well, what doesn\u2019t.\nSecurity\nA research group has found a way to automatically generate attack strings that force large language models to generate harmful content. These attacks work against both open- and closed-source models. It isn\u2019t clear that AI providers can defend against them. The cybercrime syndicate Lazarus Group is running a social engineering attack against JavaScript cryptocurrency developers. Developers are invited to collaborate on a Github project that depends on malicious NPM packages.Language models are the next big thing in cybercrime. A large language model called WormGPT has been developed for use by cybercriminals. It is based on GPT-J. WormGPT is available on the dark web along with thousands of stolen ChatGPT credentials.According to research by MITRE, out-of-bounds writes are among the most dangerous security bugs. They are also the most common, and are consistently at the top of the list. An easy solution to the problem is to use Rust.\nWeb\nAnother web framework? Enhance claims to be HTML-first, with JavaScript only if you need it. The reality may not be that simple, but if nothing else, it\u2019s evidence of growing dissatisfaction with complex and bloated web applications.Another new browser? Arc rethinks the browsing experience with the ability to switch between groups of tabs and customize individual websites. HTMX provides a way of using HTML attributes to build many advanced web page features, including WebSockets and what we used to call Ajax. All the complexity appears to be packaged into one JavaScript library.There is a law office in the Metaverse, along with a fledgling Metaverse Bar Association. It\u2019s a good place for meetings, although lawyers cannot be licensed to practice in the Metaverse.The European Court of Justice (CJEU) has ruled that Meta\u2019s approach to GDPR compliance is illegal. Meta may not use data for anything other than core functionality without explicit, freely-given consent; consent hidden in the terms of use document does not suffice.\nCryptocurrency\nGoogle has updated its policy on Android apps to allow apps to give blockchain-based assets such as NFTs.ChatGPT can be programmed to send Bitcoin payments. As the first commenter points out, this is a fairly simple application of Langchain. But it\u2019s something that was certainly going to happen. But it begs the question: when will we have GPT-based cryptocurrency arbitrage?\nBiology\nGoogle has developed Med-PaLM M, an attempt at building a \u201cgeneralist\u201d multimodal AI that has been trained for biomedical applications. Med-PaLM M is still a research project, but may represent a step forward in the application of large language models to medicine.\nMaterials\nRoom temperature ambient pressure superconductors: This claim has met with a lot of skepticism\u2014but as always, it\u2019s best to wait until another team succeeds or fails to duplicate the results. If this research holds up, it\u2019s a huge step forward.",
  "link": "https://www.oreilly.com/radar/radar-trends-to-watch-august-2023/"
 },
 {
  "title": "Real-Real-World Programming with ChatGPT",
  "content": "If you\u2019re reading this, chances are you\u2019ve played around with using AI tools like ChatGPT or GitHub Copilot to write code for you. Or even if you haven\u2019t yet, then you\u2019ve at least heard about these tools in your newsfeed over the past year. So far I\u2019ve read a gazillion blog posts about people\u2019s experiences with these AI coding assistance tools. These posts often recount someone trying ChatGPT or Copilot for the first time with a few simple prompts, seeing how it does for some small self-contained coding tasks, and then making sweeping claims like \u201cWOW this exceeded all my highest hopes and wildest dreams, it\u2019s going to replace all programmers in five years!\u201d or \u201cha look how incompetent it is \u2026 it couldn\u2019t even get my simple question right!\u201d\nI really wanted to go beyond these quick gut reactions that I\u2019ve seen so much of online, so I tried using ChatGPT for a few weeks to help me implement a hobby software project and took notes on what I found interesting. This article summarizes what I learned from that experience. The inspiration (and title) for it comes from Mike Loukides\u2019 Radar article on Real World Programming with ChatGPT, which shares a similar spirit of digging into the potential and limits of AI tools for more realistic end-to-end programming tasks.\nSetting the Stage: Who Am I and What Am I Trying to Build?\nI\u2019m a professor who is interested in how we can use LLMs (Large Language Models) to teach programming. My student and I recently published a research paper on this topic, which we summarized in our Radar article Teaching Programming in the Age of ChatGPT. Our paper reinforces the growing consensus that LLM-based AI tools such as ChatGPT and GitHub Copilot can now solve many of the small self-contained programming problems that are found in introductory classes. For instance, problems like \u201cwrite a Python function that takes a list of names, splits them by first and last name, and sorts by last name.\u201d It\u2019s well-known that current AI tools can solve these kinds of problems even better than many students can. But there\u2019s a huge difference between AI writing self-contained functions like these and building a real piece of software end-to-end. I was curious to see how well AI could help students do the latter, so I wanted to first try doing it myself.\nI needed a concrete project to implement with the help of AI, so I decided to go with an idea that had been in the back of my head for a while now: Since I read a lot of research papers for my job, I often have multiple browser tabs open with the PDFs of papers I\u2019m planning to read. I thought it would be cool to play music from the year that each paper was written while I was reading it, which provides era-appropriate background music to accompany each paper. For instance, if I\u2019m reading a paper from 2019, a popular song from that year could start playing. And if I switch tabs to view a paper from 2008, then a song from 2008 could start up. To provide some coherence to the music, I decided to use Taylor Swift songs since her discography covers the time span of most papers that I typically read: Her main albums were released in 2006, 2008, 2010, 2012, 2014, 2017, 2019, 2020, and 2022. This choice also inspired me to call my project Swift Papers.\nSwift Papers felt like a well-scoped project to test how well AI handles a realistic yet manageable real-world programming task. Here\u2019s how I worked on it: I subscribed to ChatGPT Plus and used the GPT-4 model in ChatGPT (first the May 12, 2023 version, then the May 24 version) to help me with design and implementation. I also installed the latest VS Code (Visual Studio Code) with GitHub Copilot and the experimental Copilot Chat plugins, but I ended up not using them much. I found it easier to keep a single conversational flow within ChatGPT rather than switching between multiple tools. Lastly, I tried not to search for help on Google, Stack Overflow, or other websites, which is what I would normally be doing while programming. In sum, this is me trying to simulate the experience of relying as much as possible on ChatGPT to get this project done.\nGetting Started: Setup Trials and Tribulations\nHere\u2019s the exact prompt I used to start my conversation with ChatGPT using GPT-4:\nAct as a software developer to help me build something that will play music from a time period that matches when an academic paper I am reading in the browser was written.\nI purposely kept this prompt high-level and underspecified since I wanted ChatGPT to guide me toward design and implementation ideas without me coming in with preconceived notions.\nChatGPT immediately suggested a promising direction\u2014making a browser extension that gets the date of the research paper PDF in the currently-active tab and calls a music streaming API to play a song from that time period. Since I already had a YouTube Music account, I asked whether I could use it, but ChatGPT said that YouTube Music doesn\u2019t have an API. We then brainstormed alternative ideas like using a browser automation tool to programmatically navigate and click on parts of the YouTube Music webpage. ChatGPT gave me some ideas along these lines but warned me that, \u201cIt\u2019s important to note that while this approach doesn\u2019t use any official APIs, it\u2019s more brittle and more subject to break if YouTube Music changes their website structure. [\u2026] keep in mind that web scraping and browser automation can be complex, and handling all of the edge cases can be a significant amount of work. [\u2026] using APIs might be a more reliable and manageable solution.\u201d That warning convinced me to drop this idea. I recalled that ChatGPT had recommended the Spotify Web API in an earlier response, so I asked it to teach me more about what it can do and tell me why I should use it rather than YouTube Music. It seemed like Spotify had what I needed, so I decided to go with it. I liked how ChatGPT helped me work through the tradeoffs of these initial design decisions before diving head-first into coding.\nNext we worked together to set up the boilerplate code for a Chrome browser extension, which I\u2019ve never made before. ChatGPT started by generating a manifest.json file for me, which holds the configuration settings that every Chrome extension needs. I didn\u2019t know it at the time, but manifest.json would cause me a bunch of frustration later on. Specifically:\nChatGPT generated a manifest.json file in the old Version 2 (v2) format, which is unsupported in the current version of Chrome. For a few years now Google has been transitioning developers to v3, which I didn\u2019t know about since I had no prior experience with Chrome extensions. And ChatGPT didn\u2019t warn me about this. I guessed that maybe ChatGPT only knew about v2 since it was trained on open-source code from before September 2021 (its knowledge cutoff date) and v2 was the dominant format before that date. When I tried loading the v2 manifest.json file into Chrome and saw the error message, I told ChatGPT \u201cGoogle says that manifest version 2 is deprecated and to upgrade to version 3.\u201d To my surprise, it knew about v3 from its training data and generated a v3 manifest file for me in response. It even told me that v3 is the currently-supported version (not v2!) \u2026 yet it still defaulted to v2 without giving me any warning! This frustrated me even more than if ChatGPT had not known about v3 in the first place (in that case I wouldn\u2019t blame it for not telling me something that it clearly didn\u2019t know). This theme of sub-optimal defaults will come up repeatedly\u2014that is, ChatGPT \u2018knows\u2019 what the optimal choice is but won\u2019t generate it for me without me asking for it. The dilemma is that someone like me who is new to this area wouldn\u2019t even know what to ask for in the first place.After I got the v3 manifest working in Chrome, as I tried using ChatGPT to help me add more details to my manifest.json file, it tended to \u201cdrift\u201d back to generating code in v2 format. I had to tell it a few times to only generate v3 code from now on, and I still didn\u2019t fully trust it to follow my directive. Besides generating code for v2 manifest files, it also generated starter JavaScript code for my Chrome extension that works only with v2 instead of v3, which led to more mysterious errors. If I were to start over knowing what I do now, my initial prompt would have sternly told ChatGPT that I wanted to make an extension using v3, which would hopefully avoid it leading me down this v2 rabbit hole.The manifest file that ChatGPT generated for me declared the minimal set of permissions\u2014it only listed the activeTab permission, which grants the extension limited access to the active browser tab. While this has the benefit of respecting user privacy by minimizing permissions (which is a best practice that ChatGPT may have learned from its training data), it made my coding efforts a lot more painful since I kept running into unexpected errors when I tried adding new functionality to my Chrome extension. Those errors often showed up as something not working as intended, but Chrome wouldn\u2019t necessarily display a permission denied message. In the end, I had to add four additional permissions\u2014\u201dtabs\u201d,\u00a0 \u201cstorage\u201d, \u201cscripting\u201d, \u201cidentity\u201d\u2014as well as a separate \u201chost_permissions\u201d field to my manifest.json.\nWrestling with all these finicky details of manifest.json before I could begin any real coding felt like death by a thousand cuts. In addition, ChatGPT generated other starter code in the chat, which I copied into new files in my VS Code project:\n\nIntermission 1: ChatGPT as a Personalized Tutor\nAs shown above, a typical Chrome extension like mine has at least three JavaScript files: a background script, a content script, and a pop-up script. At this point I wanted to learn more about what all these files are meant to do rather than continuing to obediently copy-paste code from ChatGPT into my project. Specifically, I discovered that each file has different permissions for what browser or page components it can access, so all three must coordinate to make the extension work as intended. Normally I would read tutorials about how this all fits together, but the problem with tutorials is that they are not customized to my specific use case. Tutorials provide generic conceptual explanations and use made-up toy examples that I can\u2019t relate to. So I end up needing to figure out how their explanations may or may not apply to my own context.\nIn contrast, ChatGPT can generate personalized tutorials that use my own Swift Papers project as the example in its explanations! For instance, when it explained to me what a content script does, it added that \u201cFor your specific project, a content script would be used to extract information (the publication date) from the academic paper\u2019s webpage. The content script can access the DOM of the webpage, find the element that contains the publication date, and retrieve the date.\u201d Similarly, it taught me that \u201cBackground scripts are ideal for handling long-term or ongoing tasks, managing state, maintaining databases, and communicating with remote servers. In your project, the background script could be responsible for communicating with the music API, controlling the music playback, and storing any data or settings that need to persist between browsing sessions.\u201d\nI kept asking ChatGPT follow-up questions to get it to teach me more nuances about how Chrome extensions worked, and it grounded its explanations in how those concepts applied to my Swift Papers project. To accompany its explanations, it also generated relevant example code that I could try out by running my extension. These explanations clicked well in my head because I was already deep into working on Swift Papers. It was a much better learning experience than, say, reading generic getting-started tutorials that walk through creating example extensions like \u201ctrack your page reading time\u201d or \u201cremove clutter from a webpage\u201d or \u201cmanage your tabs better\u201d \u2026 I couldn\u2019t bring myself to care about those examples since THEY WEREN\u2019T RELEVANT TO ME! At the time, I cared only about how these concepts applied to my own project, so ChatGPT shined here by generating personalized mini-tutorials on-demand.\nAnother great side-effect of ChatGPT teaching me these concepts directly within our ongoing chat conversation is that whenever I went back to work on Swift Papers after a few days away from it, I could scroll back up in the chat history to review what I recently learned. This reinforced the knowledge in my head and got me back into the context of resuming where I last left off. To me, this is a huge benefit of a conversational interface like ChatGPT versus an IDE autocomplete interface like GitHub Copilot, which doesn\u2019t leave a trace of its interaction history. Even though I had Copilot installed in VS Code as I was working on Swift Papers, I rarely used it (beyond simple autocompletions) since I liked having a chat history in ChatGPT to refer back to in later sessions.\nNext Up: Choosing and Installing a Date Parsing Library\nIdeally Swift Papers would infer the date when an academic paper was written by analyzing its PDF file, but that seemed too hard to do since there isn\u2019t a standard place within a PDF where the publication date is listed. Instead what I decided to do was to parse the \u201clanding pages\u201d for each paper that contains metadata such as its title, abstract, and publication date. Many papers I read are linked from a small handful of websites, such as the ACM Digital Library, arXiv, or Google Scholar, so I could parse the HTML of those landing pages to extract publication dates. For instance, here\u2019s the landing page for the classic Beyond being there paper:\n\nI wanted to parse the \u201cPublished: 01 June 1992\u201d string on that page to get 1992 as the publication year. I could\u2019ve written this code by hand, but I wanted to try using a JavaScript date parsing library since it would be more robust to date format variations that appear on various websites (e.g., using \u201c22\u201d for the year 2022). Also, since any real-world software project will need to use external libraries, I wanted to see how well ChatGPT could help me choose and install libraries.\nChatGPT suggested two libraries for me: Moment.js and chrono-node. However, it warned me about Moment.js: \u201cas of September 2020, it is considered a legacy project and not recommended for new projects as the team is not planning on doing any new development or maintenance.\u201d I verified this was true by seeing the same warning on the Moment.js homepage. But still, I liked how Moment.js was available as a single self-contained file that I could directly include into my extension without using a package manager like npm or a bundler like webpack (the fewer external tools I needed to set up, the better!). Or so I thought \u2026 ChatGPT led me to believe that I could get by without npm and webpack, but later I discovered that this only works in the old Manifest v2 extension format that is no longer supported by Chrome. It turns out that with Manifest v3, all external libraries must be bundled together using tools like npm and webpack in order for the extension to import them. So it goes\u2026 once more I got bitten by the fact that ChatGPT was biased toward producing outdated information for v2 rather than the newer v3, presumably because there was a lot more information about v2 in its training data.\nSince I had to use npm and webpack anyways, I decided to go with chrono-node since it seemed more robust and up-to-date (no pun intended). I had ChatGPT help me set up my webpack configuration file (webpack.config.js) and things almost seemed to work, except that I got a mysterious error. When I pasted the error message into ChatGPT, it correctly diagnosed the problem, which was something related to (surprise surprise!) webpack\u2019s default settings not being compatible with the security restrictions of my extension\u2019s Manifest v3 format. It also suggested a working fix to webpack.config.js:\nThis error message is related to the Content Security Policy (CSP) of Chrome extensions. [\u2026] By default, Manifest V3 disallows the use of eval() and the Function() constructor, which are typically used for evaluating or creating code from strings [\u2026] Webpack, by default, includes a small amount of inline JavaScript code for handling hot module replacement and some other features. This code uses eval() which is not allowed in the context of a Chrome extension. To solve this, you can adjust your webpack configuration to output code in a format that doesn\u2019t rely on eval(). In your webpack.config.js, you can set the devtool option to \u2018none\u2019 or use the \u2018source-map\u2019 setting [\u2026]\nHere again ChatGPT showed me that it clearly knew what the problem was (since it told me after I fed it the error message!) and how to fix it. So why didn\u2019t it produce the correct webpack configuration file in the first place?\nMore generally, several times I\u2019ve seen ChatGPT produce code that I felt might be incorrect. Then when I tell it that there might be a bug in a certain part, it admits its mistake and produces the correct code in response. If it knew that its original code was incorrect, then why didn\u2019t it generate the correct code in the first place?!? Why did I have to ask it to clarify before it admitted its mistake? I\u2019m not an expert at how LLMs work internally, but my layperson guess is that it may have to do with the fact that ChatGPT generates code linearly one token at a time, so it may get \u2018stuck\u2019 near local maxima (with code that mostly works but is incorrect in some way) while it is navigating the enormous abstract space of possible output code tokens; and it can\u2019t easily backtrack to correct itself as it generates code in a one-way linear stream. But after it finishes generating code, when the user asks it to review that code for possible errors, it can now \u201csee\u201d and analyze all of that code at once. This comprehensive view of the code may enable ChatGPT to find bugs better, even if it couldn\u2019t avoid introducing those bugs in the first place due to how it incrementally generates code in a one-way stream. (This isn\u2019t an accurate technical explanation, but it\u2019s how I informally think about it.)\nIntermission 2: ChatGPT as a UX Design Consultant\nNow that I had a basic Chrome extension that could extract paper publication dates from webpages, the next challenge was using the Spotify API to play era-appropriate Taylor Swift songs to accompany these papers. But before embarking on another coding-intensive adventure, I wanted to switch gears and think more about UX (user experience). I got so caught up in the first few hours of getting my extension set up that I hadn\u2019t thought about how this app ought to work in detail. What I needed at this time was a UX design consultant, so I wanted to see if ChatGPT could play this role.\nNote that up until now I had been doing everything in one long-running chat session that focused on coding-related questions. That was great because ChatGPT was fully \u201cin the zone\u201d and had a very long conversation (spanning several hours over multiple days) to use as context for generating code suggestions and technical explanations. But I didn\u2019t want all that prior context to influence our UX discussion, so I decided to begin again by starting a brand-new session with the following prompt:\nYou are a Ph.D. graduate in Human-Computer Interaction and now a senior UX (user experience) designer at a top design firm. Thus, you are very familiar with both the experience of reading academic papers in academia and also designing amazing user experiences in digital products such as web applications. I am a professor who is creating a Chrome Extension for fun in order to prototype the following idea: I want to make the experience of reading academic papers more immersive by automatically playing Taylor Swift songs from the time period when each paper was written while the reader is reading that particular paper in Chrome. I have already set up all the code to connect to the Spotify Web API to programmatically play Taylor Swift songs from certain time periods. I have also already set up a basic Chrome Extension that knows what webpages the user has open in each tab and, if it detects that a webpage may contain metadata about an academic paper then it parses that webpage to get the year the paper was written in, in order to tell the extension what song to play from Spotify. That is the basic premise of my project.Your job is to serve as a UX design consultant to help me design the user experience for such a Chrome Extension. Do not worry about whether it is feasible to implement the designs. I am an experienced programmer so I will tell you what ideas are or are not feasible to implement. I just want your help with thinking through UX design.\nAs our session progressed, I was very impressed with ChatGPT\u2019s ability to help me brainstorm how to handle different user interaction scenarios. That said, I had to give it some guidance upfront using my knowledge of UX design: I started by asking it to come up with a few user personas and then to build up some user journeys for each. Given this initial prompting, ChatGPT was able to help me come up with practical ideas that I didn\u2019t originally consider all too well, especially for handling unusual edge cases (e.g., what should happen to the music when the user switches between tabs very quickly?). The back-and-forth conversational nature of our chat made me feel like I was talking to a real human UX design consultant.\nI had a lot of fun working with ChatGPT to refine my initial high-level ideas into a detailed plan for how to handle specific user interactions within Swift Papers. The culmination of our consulting session was ChatGPT generating ASCII diagrams of user journeys through Swift Papers, which I could later refer to when implementing this logic in code. Here\u2019s one example:\n\nReflecting back, this session was productive because I was familiar enough with UX design concepts to steer the conversation towards more depth. Out of curiosity, I started a new chat session with exactly the same UX consultant prompt as above but then played the part of a total novice instead of guiding it:\nI don\u2019t know anything about UX design. Can you help me get started since you are the expert?\nThe conversation that followed this prompt was far less useful since ChatGPT ended up giving me a basic primer on UX Design 101 and offering high-level suggestions for how I can start thinking about the user experience of Swift Papers. I didn\u2019t want to nudge it too hard since I was pretending to be a novice, and it wasn\u2019t proactive enough to ask me clarifying questions to probe deeper. Perhaps if I had prompted it to be more proactive at the start, then it could have elicited more information even from a novice.\nThis digression reinforces the widely-known consensus that what you get out of LLMs like ChatGPT is only as good as the prompts you\u2019re able to put in. There\u2019s all of this relevant knowledge hiding inside its neural network mastermind of billions and billions of LLM parameters, but it\u2019s up to you to coax it into revealing what it knows by taking the lead in conversations and crafting the right prompts to direct it toward useful responses. Doing so requires a degree of expertise in the domain you\u2019re asking about, so it\u2019s something that beginners would likely struggle with.\nThe Last Big Hurdle: Working with the Spotify API\nAfter ChatGPT helped me with UX design, the last hurdle I had to overcome was figuring out how to connect my Chrome extension to the Spotify Web API to select and play music. Like my earlier adventure with installing a date parsing library, connecting to web APIs is another common real-world programming task, so I wanted to see how well ChatGPT could help me with it.\nThe gold standard here is an expert human programmer who has a lot of experience with the Spotify API and who is good at teaching novices. ChatGPT was alright for getting me started but ultimately didn\u2019t meet this standard. My experience here showed me that human experts still outperform the current version of ChatGPT along the following dimensions:\nContext, context, context: Since ChatGPT can\u2019t \u201csee\u201d my screen, it lacks a lot of useful task context that a human expert sitting beside me would have. For instance, connecting to a web API requires a lot of \u201cpointing-and-clicking\u201d manual setup work that isn\u2019t programming: I had to register for a paid Spotify Premium account to grant me API access, navigate through its web dashboard interface to create a new project, generate API keys and insert them into various places in my code, then register a URL where my app lives in order for authentication to work. But what URL do I use? Swift Papers is a Chrome extension running locally on my computer rather than online, so it doesn\u2019t have a real URL. I later discovered that Chrome extensions export a fake chromiumapp.org URL that can be used for web API authentication. A human expert who is pair programming with me would know all these ultra-specific idiosyncrasies and guide me through pointing-and-clicking on the various dashboards to put all the API keys and URLs in the right places. In contrast, since ChatGPT can\u2019t see this context, I have to explicitly tell it what I want at each step. And since this setup process was so new to me, I had a hard time thinking about how to phrase my questions. A human expert would be able to see me struggling and step in to offer proactive assistance for getting me unstuck.Bird\u2019s-eye view: A human expert would also understand what I\u2019m trying to do\u2014selecting and playing date-appropriate songs\u2014and guide me on how to navigate the labyrinth of the sprawling Spotify API in order to do it. In contrast, ChatGPT doesn\u2019t seem to have as much of a bird\u2019s-eye view, so it eagerly barrels ahead to generate code with specific low-level API calls whenever I ask it something. I, too, am eager to follow its lead since it sounds so confident each time it suggests code along with a convincing explanation (LLMs tend to adopt an overconfident tone, even if their responses may be factually inaccurate). That sometimes leads me on a wild goose chase down one direction only to realize that it\u2019s a dead-end and that I have to backtrack. More generally, it seems hard for novices to learn programming in this piecemeal way by churning through one ChatGPT response after another rather than having more structured guidance from a human expert.Tacit (unwritten) knowledge: The Spotify API is meant to control an already-open Spotify player (e.g., the web player or a dedicated app), not to directly play songs. Thus, ChatGPT told me it was not possible to use it to play songs in the current browser tab, which Swift Papers needed to do. I wanted to verify this for myself, so I went back to \u201cold-school\u201d searching the web, reading docs, and looking for example code online. I found that there was conflicting and unreliable information about whether it\u2019s even possible to do this. And since ChatGPT is trained on text from the internet, if that text doesn\u2019t contain high-quality information about a topic, then ChatGPT won\u2019t work well for it either. In contrast, a human expert can draw upon their vast store of experience from working with the Spotify API in order to teach me tricks that aren\u2019t well-documented online. In this case, I eventually figured out a hack to get playback working by forcing a Spotify web player to open in a new browser tab, using a super-obscure and not-well-documented API call to make that player \u2018active\u2019 (or else it sometimes won\u2019t respond to requests to play \u2026 that took me forever to figure out, and ChatGPT kept giving me inconsistent responses that didn\u2019t work), and then playing music within that background tab. I feel that humans are still better than LLMs at coming up with these sorts of hacks since there aren\u2019t readily-available online resources to document them. A lot of this hard-earned knowledge is tacit and not written down anywhere, so LLMs can\u2019t be trained on it.Lookahead: Lastly, even in instances when ChatGPT could help out by generating good-quality code, I often had to manually update other source code files to make them compatible with the new code that ChatGPT was giving me. For instance, when it suggested an update to a JavaScript file to call a specific Chrome extension API function, I also had to modify my manifest.json to grant an additional permission before that function call could work (bitten by permissions again!). If I didn\u2019t know to do that, then I would see some mysterious error message pop up, paste it into ChatGPT, and it would sometimes give me a way to fix it. Just like earlier, ChatGPT \u201cknows\u201d the answer here, but I must ask it the right question at every step along the way, which can get exhausting. This is especially a problem for novices since we often don\u2019t know what we don\u2019t know, so we don\u2019t know what to even ask for in the first place! In contrast, a human expert who is helping me would be able to \u201clook ahead\u201d a few steps based on their experience and tell me what other files I need to edit ahead of time so I don\u2019t get bitten by these bugs in the first place.\nIn the end I got this Spotify API setup working by doing some old-fashioned web searching to supplement my ChatGPT conversation. (I did try the ChatGPT + Bing web search plugin for a bit, but it was slow and didn\u2019t produce useful results, so I couldn\u2019t tolerate it any more and just shut it off.) The breakthrough came as I was browsing a GitHub repository of Spotify Web API example code. I saw an example for Node.js that seemed to do what I wanted, so I copy-pasted that code snippet into ChatGPT and told it to adapt the example for my Swift Papers app (which isn\u2019t using Node.js):\nHere\u2019s some example code using Implicit Grant Flow from Spotify\u2019s documentation, which is for a Node.js app. Can you adapt it to fit my chrome extension? [I pasted the code snippet here]\nChatGPT did a good job at \u201ctranslating\u201d that example into my context, which was exactly what I needed at the moment to get unstuck. The code it generated wasn\u2019t perfect, but it was enough to start me down a promising path that would eventually lead me to get the Spotify API working for Swift Papers. Reflecting back, I later realized that I had manually done a simple form of RAG (Retrieval Augmented Generation) here by using my intuition to retrieve a small but highly-relevant snippet of example code from the vast universe of all code on the internet and then asking a super-specific question about it. (However, I\u2019m not sure a beginner would be able to scour the web to find such a relevant piece of example code like I did, so they would probably still be stuck at this step because ChatGPT alone wasn\u2019t able to generate working code without this extra push from me.)\nEpilogue: What Now?\nI have a confession: I didn\u2019t end up finishing Swift Papers. Since this was a hobby project, I stopped working on it after about two weeks when my day-job got more busy. However, I still felt like I completed the initial hard parts and got a sense of how ChatGPT could (and couldn\u2019t) help me along the way. To recap, this involved:\nSetting up a basic Chrome extension and familiarizing myself with the concepts, permission settings, configuration files, and code components that must coordinate together to make it all work.Installing third-party JavaScript libraries (such as a date parsing library) and configuring the npm and webpack toolchain so that these libraries work with Chrome extensions, especially given the strict security policies of Manifest v3.Connecting to the Spotify Web API in such a way to support the kinds of user interactions that I needed in Swift Papers and dealing with the idiosyncrasies of accessing this API via a Chrome extension.Sketching out detailed UX journeys for the kinds of user interactions to support and how Swift Papers can handle various edge cases.\nAfter laying this groundwork, I was able to start getting into the flow of an edit-run-debug cycle where I knew exactly where to add code to implement a new feature, how to run it to assess whether it did what I intended, and how to debug. So even though I stopped working on this project due to lack of time, I got far enough to see how completing Swift Papers would be \u201cjust a matter of programming.\u201d Note that I\u2019m not trying to trivialize the challenges involved in programming, since I\u2019ve done enough of it to know that the devil is in the details. But these coding-specific details are exactly where AI tools like ChatGPT and GitHub Copilot shine! So even if I had continued adding features throughout the coming weeks, I don\u2019t feel like I would\u2019ve gotten any insights about AI tools that differ from what many others have already written about. That\u2019s because once the software environment has been set up (e.g., libraries, frameworks, build systems, permissions, API authentication keys, and other plumbing to hook things together), then the task at hand reduces to a self-contained and well-defined programming problem, which AI tools excel at.\nIn sum, my goal in writing this article was to share my experiences using ChatGPT for the more open-ended tasks that came before my project turned into \u201cjust a matter of programming.\u201d Now, some may argue that this isn\u2019t \u201creal\u201d programming since it feels like just a bunch of mundane setup and configuration work. But I believe that if \u201creal-world\u201d programming means creating something realistic with code, then \u201creal-real-world\u201d programming (the title of this article!) encompasses all these tedious and idiosyncratic errands that are necessary before any real programming can begin. And from what I\u2019ve experienced so far, this sort of work isn\u2019t something humans can fully outsource to AI tools yet. Long story short, someone today can\u2019t just give AI a high-level description of Swift Papers and have a robust piece of software magically pop out the other end. I\u2019m sure people are now working on the next generation of AI that can bring us closer to this goal (e.g., much longer context windows with Claude 2 and retrieval augmented generation with Cody), so I\u2019m excited to see what\u2019s in store. Perhaps future AI tool developers could use Swift Papers as a benchmark to assess how well their tool performs on an example real-real-world programming task. Right now, widely-used benchmarks for AI code generation (e.g., HumanEval, MBPP) consist of small self-contained tasks that appear in introductory classes, coding interviews, or programming competitions. We need more end-to-end, real-world benchmarks to drive improvements in these AI tools.\nLastly, switching gears a bit, I also want to think more in the future about how AI tools can teach novices the skills they need to create realistic software projects like Swift Papers rather than doing all the implementation work for them. At present, ChatGPT and Copilot are reasonably good \u201cdoers\u201d but not nearly as good at being teachers. This is unsurprising since they were designed to carry out instructions like a good assistant would, not to be an effective teacher who provides pedagogically-meaningful guidance. With the proper prompting and fine-tuning, I\u2019m sure they can do much better here, and organizations like Khan Academy are already customizing GPT-4 to become a personalized tutor. I\u2019m excited to see how things progress in this fast-moving space in the coming months and years. In the meantime, for more thoughts about AI coding tools in education, check out this other recent Radar article that I co-authored, Teaching Programming in the Age of ChatGPT, which summarizes our research paper about this topic.",
  "link": "https://www.oreilly.com/radar/real-real-world-programming-with-chatgpt/"
 },
 {
  "title": "Teaching Programming in the Age of ChatGPT",
  "content": "Imagine for a minute that you\u2019re a programming instructor who\u2019s spent many hours making creative homework problems to introduce your students to the world of programming. One day, a colleague tells you about an AI tool called ChatGPT. To your surprise (and alarm), when you give it your homework problems, it solves most of them perfectly, maybe even better than you can! You realize that by now, AI tools like ChatGPT and GitHub Copilot are good enough to solve all of your class\u2019s homework problems and affordable enough that any student can use them. How should you teach students in your classes knowing that these AI tools are widely available?\nI\u2019m Sam Lau from UC San Diego, and my Ph.D. advisor (and soon-to-be faculty colleague) Philip Guo and I are presenting a research paper at the International Computing Education Research conference (ICER) on this very topic. We wanted to know:\nHow are computing instructors planning to adapt their courses as more and more students start using AI coding assistance tools such as ChatGPT and GitHub Copilot?\nTo answer this question, we gathered a diverse sample of perspectives by interviewing 20 introductory programming instructors at universities across 9 countries (Australia, Botswana, Canada, Chile, China, Rwanda, Spain, Switzerland, United States) spanning all 6 populated continents. To our knowledge, our paper is the first empirical study to gather instructor perspectives about these AI coding tools that more and more students will likely have access to in the future.\nHere\u2019s a summary of our findings:\n\nShort-Term Plans: Instructors Want to Stop Students from Cheating\nEven though we didn\u2019t specifically ask about cheating in our interviews, all of the instructors we interviewed mentioned it as a primary reason to make changes to their courses in the short term. Their reasoning was: If students could easily get answers to their homework questions using AI tools, then they won\u2019t need to think deeply about the material, and thus won\u2019t learn as much as they should. Of course, having an answer key isn\u2019t a new problem for instructors, who have always worried about students copying off each other or online resources like Stack Overflow. But AI tools like ChatGPT generate code with slight variations between responses, which is enough to fool most plagiarism detectors that instructors have available today.\nThe deeper issue for instructors is that if AI tools can easily solve problems in introductory courses, students who are learning programming for the first time might be led to believe that AI tools can correctly solve any programming task, which can cause them to grow overly reliant on them. One instructor described this as not just cheating, but \u201ccheating badly\u201d because AI tools generate code that\u2019s incorrect in subtle ways that students might not be able to understand.\nTo discourage students from becoming over-reliant on AI tools, instructors used a mix of strategies, including making exams in-class and on-paper, and also having exams count for more of students\u2019 final grades. Some instructors also explicitly banned AI tools in class, or exposed students to the limitations of AI tools. For example, one instructor copied old homework questions into ChatGPT as a live demo in a lecture and asked students to critique the strengths and weaknesses of the AI-generated code. That said, instructors considered these strategies short-term patches; the sudden appearance of ChatGPT at the end of 2022 meant that instructors needed to make adjustments before their courses started in 2023, which was when we interviewed them for our study.\nLonger-Term Plans (Part 1): Ideas to Resist AI Tools\nIn the next part of our study, instructors brainstormed many ideas about how to approach AI tools longer-term. We split up these ideas into two main categories: ideas that resist AI tools, and ideas that embrace them. Do note that most instructors we interviewed weren\u2019t completely on one side or the other\u2014they shared a mix of ideas from both categories. That said, let\u2019s start with why some instructors talked about resisting AI tools, even in the longer term.\nThe most common reason for wanting to resist AI tools was the concern that students wouldn\u2019t learn the fundamentals of programming. Several instructors drew an analogy to using a calculator in math class: using AI tools could be like, in the words of one of our interview participants, \u201cgiving kids a calculator and they can play around with a calculator, but if they don\u2019t know what a decimal point means, what do they really learn or do with it? They may not know how to plug in the right thing, or they don\u2019t know how to interpret the answer.\u201d Others mentioned ethical objections to AI. For example, one instructor was worried about recent lawsuits around Copilot\u2019s use of open-source code as training data without attribution. Others shared concerns over the training data bias for AI tools.\nTo resist AI tools practically, instructors proposed ideas for designing \u201cAI-proof\u201d homework assignments, for example, by using a custom-built library for their course. Also, since AI tools are typically trained on U.S./English-centric data, instructors from other countries thought that they could make their assignments harder for AI to solve by including local cultural and language context (e.g. slang) from their countries.\nInstructors also brainstormed ideas for AI-proof assessments. One common suggestion was to use in-person paper exams since proctors could better ensure that students were only using paper and pencil. Instructors also mentioned that they could try oral exams where students either talk to a course staff member in-person, or record a video explaining what their code does. Although these ideas were first suggested to help keep assessments meaningful, instructors also pointed out that these assessments could actually improve pedagogy by giving students a reason to think more deeply about why their code works rather than simply trying to get code that produces a correct answer.\nLonger-Term Plans (Part 2): Ideas to Embrace AI Tools\nAnother group of ideas sought to embrace AI tools in introductory programming courses. The instructors we interviewed mentioned several reasons for wanting this future. Most commonly, instructors felt that AI coding tools would become standard for programmers; since \u201cit\u2019s inevitable\u201d that professionals will use AI tools on the job, instructors wanted to prepare students for their future jobs. Related to this, some instructors thought that embracing AI tools could make their institutions more competitive by getting ahead of other universities that were more hesitant about doing so.\nInstructors also saw potential learning benefits to using AI tools. For example, if these tools make it so that students don\u2019t need to spend as long wrestling with programming syntax in introductory courses, students could spend more time learning about how to better design and engineer programs. One instructor drew an analogy to compilers: \u201cWe don\u2019t need to look at 1\u2019s and 0\u2019s anymore, and nobody ever says, \u2018Wow what a big problem, we don\u2019t write machine language anymore!\u2019 Compilers are already like AI in that they can outperform the best humans in generating code.\u201d And in contrast to concerns that AI tools could harm equity and access, some instructors thought that they could make programming less intimidating and thus more accessible by letting students start coding using natural language.\nInstructors also saw many potential ways to use AI tools themselves. For example, many taught courses with over a hundred students, where it would be too time-consuming to give individual feedback to each student. Instructors thought that AI tools trained on their class\u2019s data could potentially give personalized help to each student, for example by explaining why a piece of code doesn\u2019t work. Instructors also thought AI tools could help generate small practice problems for their students.\nTo prepare students for a future where AI tools are widespread, instructors mentioned that they could spend more time in class on code reading and critique rather than writing code from scratch. Indeed, these skills could be useful in the workplace even today, where programmers spend significant amounts of time reading and reviewing other people\u2019s code. Instructors also thought that AI tools gave them the opportunity to give more open-ended assignments, and even have students collaborate with AI directly on their work, where an assignment would ask students to generate code using AI and then iterate on the code until it was both correct and efficient.\nReflections\nOur study findings capture a rare snapshot in time in early 2023 as computing instructors are just starting to form opinions about this fast-growing phenomenon but have not yet converged to any consensus about best practices. Using these findings as inspiration, we synthesized a diverse set of open research questions regarding how to develop, deploy, and evaluate AI coding tools for computing education. For instance, what mental models do novices form both about the code that AI generates and about how the AI works to produce that code? And how do those novice mental models compare to experts\u2019 mental models of AI code generation? (Section 7 of our paper has more examples.)\nWe hope that these findings, along with our open research questions, can spur conversations about how to work with these tools in effective, equitable, and ethical ways.\nCheck out our paper here and email us if you\u2019d like to discuss anything related to it!From \u201cBan It Till We Understand It\u201d to \u201cResistance is Futile\u201d: How University Programming Instructors Plan to Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT and GitHub Copilot. Sam Lau and Philip J. Guo. ACM Conference on International Computing Education Research (ICER), August 2023.",
  "link": "https://www.oreilly.com/radar/teaching-programming-in-the-age-of-chatgpt/"
 },
 {
  "title": "Fearing the Wrong Thing",
  "content": "There\u2019s a lot of angst about software developers \u201closing their jobs\u201d to AI, being replaced by a more intelligent version of ChatGPT, GitHub\u2019s Copilot, Google\u2019s Codey, or something similar. Matt Welsh has been talking and writing about the end of programming as such. He\u2019s asking whether large language models eliminate programming as we know it, and he\u2019s excited that the answer is \u201cyes\u201d: eventually, if not in the immediate future. But what does this mean in practice? What does this mean for people who earn their living from writing software?\nSome companies will certainly value AI as a tool for replacing human effort, rather than for augmenting human capabilities. Programmers who work for those companies risk losing their jobs to AI. If you work for one of those organizations, I\u2019m sorry for you, but it\u2019s really an opportunity. Despite the well-publicized layoffs, the job market for programmers is great, it\u2019s likely to remain great, and you\u2019re probably better off finding an employer who doesn\u2019t see you as an expense to be minimized. It\u2019s time to learn some new skills and find an employer who really values you.\nBut the number of programmers who are \u201creplaced by AI\u201d will be small.\u00a0 Here\u2019s why and how the use of AI will change the discipline as a whole. I did a very non-scientific study of the amount of time programmers actually spend writing code. OK, I just typed \u201cHow much of a software developer\u2019s time is spent coding\u201d into the search bar and looked at the top few articles, which gave percentages ranging from 10% to 40%. My own sense, from talking to and observing many people over the years, falls into the lower end of that range: 15% to 20%.\nChatGPT won\u2019t make the 20% of their time that programmers spend writing code disappear completely. You still have to write prompts, and we\u2019re all in the process of learning that if you want ChatGPT to do a good job, the prompts have to be very detailed.\u00a0How much time and effort does that save? I\u2019ve seen estimates as high as 80%, but I don\u2019t believe them; I think 25% to 50% is more reasonable. If 20% of your time is spent coding, and AI-based code generation makes you 50% more efficient, then you\u2019re really only getting about 10% of your time back. You can use it to produce more code\u2014I\u2019ve yet to see a programmer who was underworked, or who wasn\u2019t up against an impossible delivery date. Or you can spend more time on the \u201crest of the job,\u201d the 80% of your time that wasn\u2019t spent writing code. Some of that time is spent in pointless meetings, but much of \u201cthe rest of the job\u201d is understanding the user\u2019s needs, designing, testing, debugging, reviewing code, finding out what the user really needs (that they didn\u2019t tell you the first time), refining the design, building an effective user interface, auditing for security, and so on. It\u2019s a lengthy list.\nThat \u201crest of the job\u201d (particularly the \u201cuser\u2019s needs\u201d part) is something our industry has never been particularly good at. Design\u2014of the software itself, the user interfaces, and the data representation\u2014is certainly not going away, and isn\u2019t something the current generation of AI is very good at. We\u2019ve come a long way, but I don\u2019t know anyone who hasn\u2019t had to rescue code that was best described as a \u201cseething mass of bits.\u201d Testing and debugging\u2014well, if you\u2019ve played with ChatGPT much, you know that testing and debugging won\u2019t disappear. AIs generate incorrect code, and that\u2019s not going to end soon. Security auditing will only become more important, not less; it\u2019s very hard for a programmer to understand the security implications of code they didn\u2019t write. Spending more time on these things\u2014and leaving the details of pushing out lines of code to an AI\u2014will surely improve the quality of the products we deliver.\nNow, let\u2019s take a really long term view. Let\u2019s assume that Matt Welsh is right, and that programming as we know it will disappear\u2014not tomorrow, but sometime in the next 20 years. Does it really disappear? A couple of weeks ago, I showed Tim O\u2019Reilly some of my experiments with Ethan and Lilach Mollick\u2019s prompts for using AI in the classroom. His reaction was \u201cThis prompt is really programming.\u201d He\u2019s right. Writing a detailed prompt really is just a different form of programming. You\u2019re still telling a computer what you want it to do, step by step. And I realized that, after spending 20 years complaining that programming hasn\u2019t changed significantly since the 1970s, ChatGPT has suddenly taken that next step. It isn\u2019t a step towards some new paradigm, whether functional, object oriented, or hyperdimensional. I expected the next step in programming languages to be visual, but it isn\u2019t that either. It\u2019s a step towards a new kind of programming that doesn\u2019t require a formally defined syntax or semantics. Programming without virtual punch cards. Programming that doesn\u2019t require you to spend half your time looking up the names and parameters of library functions that you\u2019ve forgotten about.\nIn the best of all possible worlds, that might bring the time spent actually writing code down to zero, or close to it. But that best case only saves 20% of a programmer\u2019s time. Furthermore, it doesn\u2019t really eliminate programming. It changes it\u2014possibly making programmers more efficient, and definitely giving programmers more time to talk to users, understand the problems they face, and design good, secure systems for solving those problems. Counting lines of code is less important than understanding problems in depth and figuring out how to solve them\u2014but that\u2019s nothing new. Twenty years ago, the Agile Manifesto pointed in this direction, valuing:\nIndividuals and interactions over processes and toolsWorking software over comprehensive documentationCustomer collaboration over contract negotiationResponding to change over following a plan\nDespite 23 years of \u201cagile practices,\u201d customer collaboration has always been shortchanged. Without engaging with customers and users, Agile quickly collapses to a set of rituals. Will freeing programmers from syntax actually yield more time to collaborate with customers and respond to change?\u00a0To prepare for this future, programmers will need to learn more about working directly with customers and designing software that meets their needs. That\u2019s an opportunity, not a disaster. Programmers have labored too long under the stigma of being neckbeards who can\u2019t and shouldn\u2019t be allowed to talk to humans. It\u2019s time to reject that stereotype, and to build software as if people mattered.\nAI isn\u2019t something to be feared. Writing about OpenAI\u2019s new Code Interpreter plug-in (gradually rolling out now), Ethan Mollick says \u201cMy time becomes more valuable, not less, as I can concentrate on what is important, rather than the rote.\u201d AI is something to be learned, tested, and incorporated into programming practices so that programmers can spend more time on what\u2019s really important: understanding and solving problems. The endpoint of this revolution won\u2019t be an unemployment line; it will be better software. The only thing to be feared is failing to make that transition.\nProgramming isn\u2019t going to go away. It\u2019s going to change, and those changes will be for the better.",
  "link": "https://www.oreilly.com/radar/fearing-the-wrong-thing/"
 },
 {
  "title": "Radar Trends to Watch: July 2023",
  "content": "A surprising number of the entries for AI are about generative models that don\u2019t generate text or artwork\u2014specifically, they generate human voices or music. Is voice the next frontier for AI?\u00a0Google\u2019s AudioPaLM, which unites speech recognition, speech synthesis, and language modeling, may show the direction in which AI is heading. There\u2019s also increasing concern about the consequences of training AI on data that was generated by AI. With less input from real humans, does \u201cmodel collapse\u201d lead to output that is mediocre at best?\nAI\nRoboCat is an AI model for controlling robots that learns how to learn. Unlike most robotics, which are designed to perform a small number of tasks, RoboCat can learn new tasks after it is deployed, and the learning process speeds up as it learns more tasks.AudioPaLM is a new language model from Google that combines speech generation, speech understanding, and natural language processing. It\u2019s a large language model that understands and produces voice.Voicemod is a tool for turning human speech into AI-generated speech in real time. The company offers a number of \u201csonic avatars\u201d that can be further customized.Tree-of-thought prompting expands on chain-of-thought by causing language models to consider multiple reasoning paths in the process of generating an output.Facebook/Meta has built a new generative speech model called Voicebox that they claim surpasses the performance of other models. They have not released an open source version. The paper describes some ways to distinguish generated speech from human speech.MIT Technology Review provides a good summary of key points in the EU\u2019s draft proposal for regulating AI. It will probably take at least two years for this proposal to move through legislative channels.OpenLLM provides support for running a number of open source large language models in production. It includes the ability to integrate with tools like Bento; support for langchain is promised soon.Infinigen is a photorealistic natural-world 3D scene generator. It is designed to generate synthetic training data for AI systems. It currently generates terrains, plants, animals, and natural phenomena like weather; built objects may be added later.Facebook/Meta has created a new large model called I-JEPA (Image Joint Embedding Predictive Architecture).\u00a0It claims to be more efficient than other models, and to work by building a higher-level model of the world, as humans do. It is a first step towards implementing Yann Lecun\u2019s ideas about next-generation artificial intelligence.MusicGen is a new generative model for music from Facebook/Meta. It sounds somewhat more convincing than other music models, but it\u2019s not clear that it can do more than reassemble musical cliches.OpenAI has added a \u201cfunction calling\u201d API. The API allows an application to describe functions to the model. If GPT needs to call one of those functions, it returns a JSON object describing the function call. The application can call the function and return the result to the model.A study claims that AWS Mechanical Turk workers are using AI to do their work. Mechanical Turk is often used to generate or label training data for AI systems. What impact will the use of AI to generate training data have on future generations of AI?What happens when generative AI systems are trained on data that they\u2019ve produced? When Copilot is trained on code generated by Copilot, or GPT-4 on web content generated by GPT-4? Model collapse: the \u201clong tails\u201d of the distribution disappear, and the quality of the output suffers.FrugalGPT is an idea for reducing the cost of using large language models like GPT-4. The authors propose using pipeline of language models (GPT-J, GPT-3, and GPT-4), refining the prompt at each stage so that most of the processing is done by free or inexpensive models.Deep Mind\u2019s AlphaDev has used AI to speed up sorting algorithms. Their software worked at the assembly language level; when they were done, they converted the code to back to C++ and submitted it to the LLVM project, which has included it in the C++ standard library.An artist has used Stable Diffusion to create functional QR codes that are also works of art and posted them on Reddit.The movement to regulate AI needs to learn from nuclear non-proliferation, where the key element isn\u2019t hypothetical harms (we all know what bombs can do), but traceability and transparency. Model Cards and Datasheets for Datasets are a good start.Sam Altman talks about ChatGPT\u2019s plans, saying that it\u2019s currently compute-bound and needs more GPUs. This bottleneck is delaying features like custom fine-tuning the model, expanding the context window, and multimodality (i.e., images).Facebook/Meta\u2019s LIMA is a 65B parameter language model that\u2019s based on LLaMa, but was fine-tuned on only 1,000 carefully chosen prompts and responses, without the use of RLHF (reinforcement learning with human feedback).Some things have to happen. Gandalf is a prompt injection game; your task is to get an AI to reveal its password.\nProgramming\nLeptos is a new open source, full-stack, fully typed web framework for Rust. (How many days is it since the last Web Platform?) In the not-too-distant future, WebAssembly may replace containers; software deployed as WebAssembly is portable and much smaller.Adam Jacob talks about revitalizing DevOps with a new generation of tooling that uses insights from multiplayer games and digital twins.Alex Russell on improving web performance for the majority of users, who have midrange or low-end smartphones: JavaScript is useful, but on many sites it is a huge burden.Doug Crockford says that it\u2019s time to stop using JavaScript and move on to newer, better, next-generation programming languages.Wing is a new programming language with high-level abstractions for the cloud. The claim is that these abstractions will make it easier for AI code generation to write cloud-native programs.Simpleaichat is a Python package that simplifies writing programs that use GPT 3.5 or GPT 4.StarCoder and StarCoderBase form an open source language model for writing software (similar to Codex). It was trained on \u201c\u200b\u200ba large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process.\u201dHow do you measure developer experience? Metrics tend to be technical, ignoring personal issues like developer satisfaction, the friction they encounter day-to-day, and other aspects of lived experience.OpenChat is an open source chat console that is designed to connect to a large language model (currently GPT-*). It allows anyone to create their own customized chat bot. It supports unlimited memory (using PineconeDB), and plans to add support for other language models.WebAssembly promises to improve runtime performance and latency on both the browser and the back end. It also promises to allow developers to create packages that run in any environment: Kubernetes clusters, edge devices, etc. But this capability is still a work in progress.People have started talking about software defined cars. This is an opportunity to rethink security from the ground up\u2014or to create a much bigger attack surface.LQML is a programming language designed for prompting language models. It\u2019s an early example of a formal informal language for communicating with AI systems.Memory Spy is a web application that runs simple C programs and shows you how variables are represented in memory. Even if you aren\u2019t a C programmer, you will learn a lot about how software works. Memory Spy was created by Julia Evans, @b0rk. Julia\u2019s latest zine about how computers represent integer and floating point numbers is also well worth reading.\nAugmented and Virtual Reality\nDavid Pogue\u2019s review of Apple Vision, the $3500 AR headset: Limited in a way that\u2019s reminiscent of the first iPhone\u2014\u201cBut no headset, no device, has ever hit this high a number on the wonder scale before.\u201dApple did it: they unveiled their AR/VR goggles. They are very expensive ($3499), look something like skiing googles, and have two hours of battery life on an external battery pack. It\u2019s hard to imagine wearing them in public, though Apple may manage to make them fashionable.Apple\u2019s big challenge with the Vision Pro goggles may not be getting people to use them; it may be getting developers to write compelling apps. Merely translating 2D apps into a 3D environment isn\u2019t likely to be satisfactory. How can software really take advantage of 3D?Tim Bray\u2019s post on what Augmented Reality is, and what that will require from software developers, is a must-read. It\u2019s not Apple Vision.Hachette has created a Metaverse experience named \u201cBeyond the Pages,\u201d in part as an attempt to attract a younger audience. While the original experience was only open for two days, they have promised to schedule more.\nSecurity\nRansomware is getting faster, which means that organizations have even less time to respond to an attack. To prevent becoming a victim, focus on the basics: access controls, strong passwords, multi-factor authentication, zero trust, penetration testing, and good backups.The number of attacks against systems running in \u201cthe cloud\u201d is increasing rapidly. The biggest dangers are still errors in basic hygiene, including misconfigured identity and access management.AI Package Hallucination is a new technique for distributing malware. Ask a question that causes an AI to hallucinate a package or library. Create malware with that package name, and put it in an appropriate repository. Wait for someone else to get the same recommendation and install the malware. (This assumes AI hallucinations are consistent; I\u2019m not sure that\u2019s true.) \nWeb\nA new standard allows NFTs to contain wallets, which contain NFTs. Users build collections of related resources. In addition to gaming (a character that \u201cowns\u201d its paraphernalia), this could be used for travel (a trip that contains tickets to events) or customer loyalty programs.The W3C has announced a new web standard for secure payment confirmation.\u00a0The standard is intended to make checkout simpler and less prone to fraud.Tyler Cowen argues that cryptocurrency will play a role for transactions between AI systems. AI systems aren\u2019t allowed have their own bank accounts, and that\u2019s unlikely to change in the near future. However, as they come into wider use, they will need to make transactions.Web or mobile performance isn\u2019t discussed as much as it should be. Here\u2019s a good post on on improving Wikipedia performance by eliminating a specific blocking problem: removing unnecessary JavaScript, and optimizing what remains.\nQuantum Computing\nIBM has used their quantum computer to solve a problem in materials science. It\u2019s not yet a useful result\u2014the properties of the material had to be simplified to make the result useful\u2014but it\u2019s an important step towards real-world applications for quantum computing.",
  "link": "https://www.oreilly.com/radar/radar-trends-to-watch-july-2023/"
 },
 {
  "title": "Risk Management for AI Chatbots",
  "content": "Does your company plan to release an AI chatbot, similar to OpenAI\u2019s ChatGPT or Google\u2019s Bard? Doing so means giving the general public a freeform text box for interacting with your AI model.\nThat doesn\u2019t sound so bad, right? Here\u2019s the catch: for every one of your users who has read a \u201cHere\u2019s how ChatGPT and Midjourney can do half of my job\u201d article, there may be at least one who has read one offering \u201cHere\u2019s how to get AI chatbots to do something nefarious.\u201d They\u2019re posting screencaps as trophies on social media; you\u2019re left scrambling to close the loophole they exploited.\nWelcome to your company\u2019s new AI risk management nightmare.\nSo, what do you do? I\u2019ll share some ideas for mitigation. But first, let\u2019s dig deeper into the problem.\nOld Problems Are New Again\nThe text-box-and-submit-button combo exists on pretty much every website. It\u2019s been that way since the web form was created roughly thirty years ago. So what\u2019s so scary about putting up a text box so people can engage with your chatbot?\nThose 1990s web forms demonstrate the problem all too well. When a person clicked \u201csubmit,\u201d the website would pass that form data through some backend code to process it\u2014thereby sending an e-mail, creating an order, or storing a record in a database. That code was too trusting, though. Malicious actors determined that they could craft clever inputs to trick it into doing something unintended, like exposing sensitive database records or deleting information. (The most popular attacks were cross-site scripting and SQL injection, the latter of which is best explained in the story of \u201cLittle Bobby Tables.\u201d)\nWith a chatbot, the web form passes an end-user\u2019s freeform text input\u2014a \u201cprompt,\u201d or a request to act\u2014to a generative AI model. That model creates the response images or text by interpreting the prompt and then replaying (a probabilistic variation of) the patterns it uncovered in its training data.\nThat leads to three problems:\nBy default, that underlying model will respond to any prompt.\u00a0 Which means your chatbot is effectively a naive person who has access to all of the information from the training dataset. A rather juicy target, really. In the same way that bad actors will use social engineering to fool humans guarding secrets, clever prompts are a form of\u00a0 social engineering for your chatbot.\u00a0This kind of prompt injection can get it to say nasty things. Or reveal a recipe for napalm. Or divulge sensitive details. It\u2019s up to you to filter the bot\u2019s inputs, then.The range of potentially unsafe chatbot inputs amounts to \u201cany stream of human language.\u201d It just so happens, this also describes all possible chatbot inputs.\u00a0With a SQL injection attack, you can \u201cescape\u201d certain characters so that the database doesn\u2019t give them special treatment. There\u2019s currently no equivalent, straightforward way to render a chatbot\u2019s input safe. (Ask anyone who\u2019s done content moderation for social media platforms: filtering specific terms will only get you so far, and will also lead to a lot of false positives.)The model is not deterministic.\u00a0Each invocation of an AI chatbot is a probabilistic journey through its training data. One prompt may return different answers each time it is used. The same idea, worded differently, may take the bot down a completely different road. The right prompt can get the chatbot to reveal information you didn\u2019t even know was in there.\u00a0And when that happens, you can\u2019t really explain how it reached that conclusion.\nWhy haven\u2019t we seen these problems with other kinds of AI models, then? Because most of those have been deployed in such a way that they are only communicating with trusted internal systems.\u00a0Or their inputs pass through layers of indirection that structure and limit their shape. Models that accept numeric inputs, for example, might sit behind a filter that only permits the range of values observed in the training data.\nWhat Can You Do?\nBefore you give up on your dreams of releasing an AI chatbot, remember: no risk, no reward.\nThe core idea of risk management is that you don\u2019t win by saying \u201cno\u201d to everything. You win by understanding the potential problems ahead, then figure out how to steer clear of them. This approach reduces your chances of downside loss while leaving you open to the potential upside gain.\nI\u2019ve already described the risks of your company deploying an AI chatbot. The rewards include improvements to your products and services, or streamlined customer service, or the like. You may even get a publicity boost, because just about every other article these days is about how companies are using chatbots.\nSo let\u2019s talk about some ways to manage that risk and position you for a reward. (Or, at least, position you to limit your losses.)\nSpread the word: The first thing you\u2019ll want to do is let people in the company know what you\u2019re doing.\u00a0It\u2019s tempting to keep your plans under wraps\u2014nobody likes being told to slow down or change course on their special project\u2014but there are several people in your company who can help you steer clear of trouble. And they can do so much more for you if they know about the chatbot long before it is released.\nYour company\u2019s Chief Information Security Officer (CISO) and Chief Risk Officer will certainly have ideas. As will your legal team. And maybe even your Chief Financial Officer, PR team, and head of HR, if they have sailed rough seas in the past.\nDefine a clear terms of service (TOS) and acceptable use policy (AUP): What do you do with the prompts that people type into that text box? Do you ever provide them to law enforcement or other parties for analysis, or feed it back into your model for updates? What guarantees do you make or not make about the quality of the outputs and how people use them? Putting your chatbot\u2019s TOS front-and-center will let people know what to expect before they enter sensitive personal details or even confidential company information. Similarly, an AUP will explain what kinds of prompts are permitted.\n(Mind you, these documents will spare you in a court of law in the event something goes wrong. They may not hold up as well in the court of public opinion, as people will accuse you of having buried the important details in the fine print. You\u2019ll want to include plain-language warnings in your sign-up and around the prompt\u2019s entry box so that people can know what to expect.)\nPrepare to invest in defense: You\u2019ve allocated a budget to train and deploy the chatbot, sure. How much have you set aside to keep attackers at bay? If the answer is anywhere close to \u201czero\u201d\u2014that is, if you assume that no one will try to do you harm\u2014you\u2019re setting yourself up for a nasty surprise. At a bare minimum, you will need additional team members to establish defenses between the text box where people enter prompts and the chatbot\u2019s generative AI model. That leads us to the next step.\nKeep an eye on the model: Longtime readers will be familiar with my catchphrase, \u201cNever let the machines run unattended.\u201d An AI model is not self-aware, so it doesn\u2019t know when it\u2019s operating out of its depth.\u00a0It\u2019s up to you to filter out bad inputs before they induce the model to misbehave.\nYou\u2019ll also need to review samples of the prompts supplied by end-users (there\u2019s your TOS calling) and the results returned by the backing AI model. This is one way to catch the small cracks before the dam bursts. A spike in a certain prompt, for example, could imply that someone has found a weakness and they\u2019ve shared it with others.\nBe your own adversary: Since outside actors will try to break the chatbot, why not give some insiders a try?\u00a0Red-team exercises can uncover weaknesses in the system while it\u2019s still under development.\nThis may seem like an invitation for your teammates to attack your work. That\u2019s because it is.\u00a0Better to have a \u201cfriendly\u201d attacker uncover problems before an outsider does, no?\nNarrow the scope of audience: A chatbot that\u2019s open to a very specific set of users\u2014say, \u201clicensed medical practitioners who must prove their identity to sign up and who use 2FA to login to the service\u201d\u2014will be tougher for random attackers to access. (Not impossible, but definitely tougher.) It should also see fewer hack attempts by the registered users because they\u2019re not looking for a joyride; they\u2019re using the tool to complete a specific job.\nBuild the model from scratch (to narrow the scope of training data): You may be able to extend an existing, general-purpose AI model with your own data (through an ML technique called transfer learning). This approach will shorten your time-to-market, but also leave you to question what went into the original training data. Building your own model from scratch gives you complete control over the training data, and therefore, additional influence (though, not \u201ccontrol\u201d) over the chatbot\u2019s outputs.\nThis highlights an added value in training on a domain-specific dataset: it\u2019s unlikely that anyone would, say, trick the finance-themed chatbot BloombergGPT into revealing the secret recipe for Coca-Cola or instructions for acquiring illicit substances. The model can\u2019t reveal what it doesn\u2019t know.\nTraining your own model from scratch is, admittedly, an extreme option. Right now this approach requires a combination of technical expertise and compute resources that are out of most companies\u2019 reach. But if you want to deploy a custom chatbot and are highly sensitive to reputation risk, this option is worth a look.\nSlow down: Companies are caving to pressure from boards, shareholders, and sometimes internal stakeholders to release an AI chatbot. This is the time to remind them that a broken chatbot released this morning can be a PR nightmare before lunchtime. Why not take the extra time to test for problems? \nOnward\nThanks to its freeform input and output, an AI-based chatbot exposes you to additional risks above and beyond using other kinds of AI models. People who are bored, mischievous, or looking for fame will try to break your chatbot just to see whether they can. (Chatbots are extra tempting right now because they are novel, and \u201ccorporate chatbot says weird things\u201d makes for a particularly humorous trophy to share on social media.)\nBy assessing the risks and proactively developing mitigation strategies, you can reduce the chances that attackers will convince your chatbot to give them bragging rights.\nI emphasize the term \u201creduce\u201d here.\u00a0As your CISO will tell you, there\u2019s no such thing as a \u201c100% secure\u201d system. What you want to do is close off the easy access for the amateurs, and at least give the hardened professionals a challenge.\n\nMany thanks to Chris Butler and Michael S. Manley for reviewing (and dramatically improving) early drafts of this article. Any rough edges that remain are mine.",
  "link": "https://www.oreilly.com/radar/risk-management-for-ai-chatbots/"
 },
 {
  "title": "AI\u2019s Opaque Box Is Actually a Supply Chain",
  "content": "Understanding AI\u2019s mysterious \u201copaque box\u201d is paramount to creating explainable AI. This can be simplified by considering that AI, like all other technology, has a supply chain. Knowing what makes up the supply chain is critical to enforcing the security of the AI system, establishing trust with the consumer of the AI\u2019s output, and protecting your organization from undue risk.\nWhen pondering your approach to dissecting AI\u2019s supply chain, consider how production, shipping, delivery, and invoicing are steps in just about any supply chain, for everything that you use, from toothpaste to technology. AI models are also created and delivered via supply chains.\nSome of the steps in AI\u2019s type of supply chain can be tricky to follow, with special gotchas like technology company trade secrets, closed code, and program synthesis\u2014which is the process of AI writing its own code to improve itself. Combined with continuous machine learning cycles and deployments, reviews, and recalls, there are a lot of opportunities to bring transparency to the opaque box.\nBlockchain technology is chosen by companies like Walmart to bring transparency to supply chains like food production and delivery, because it is tamper evident and distributed. Blockchain technology is used in an enterprise stack alongside other systems, to make integrations more secure and to establish a single audit trail. Verification, including that of the identity of all participants in a blockchain network, and compliance are woven throughout the workflow and processes.\nTypically, an enterprise blockchain audit trail will consist of linked blocks containing transactions that reference hyperlinks to data that is stored off chain in traditional databases. Meanwhile, the system creates a cryptographic verification of that data and stores the verification on blockchain, which is comparable to the traditional process of providing a checksum to ensure integrity of a file download. If data on the blockchain network ever undergoes tampering, the cryptographic hash used as verification will no longer compute to the same value.\nWhen you dissect AI\u2019s supply chain, at the root, you will find algorithms. These are the mathematical formulas written to simulate functions of the brain, which underlie the AI programming. The algorithms are compiled into code libraries, and then distributed to AI developers who use them to write custom AI models. Meanwhile, a data scientist acquires and prepares training data, which is then used to bring the AI model to life.\nUniversity of Baltimore Law Professor Michelle Gillman, who fights to help people who were automatically denied benefits, recently spoke with NBC about the importance of understanding the origin of algorithms when managing AI risk. According to Gillman, whose clients often face life and death situations that are being decided by AI, \u201cI\u2019ve been in hearings where no one in the room can describe for me, how does the algorithm work? What factors does it weigh? How does it weigh those factors? And so you are really left unable to make a case for your client in those circumstances.\u201d\nNext, a workflow begins that implements an AI engineering and machine learning operations (MLOps) process, in which cycles of experiments and deployments are conducted, and the AI model, its data, and the variables, or hyperparameters of the experiment are tested, tweaked, and improved. This part of the supply chain keeps going in a cycle even after delivery to the consumer, since the training and improvement process is generally continuous. The consumer\u2019s input in the form of reviews and ratings becomes part of the process to improve the model. The stakeholders of the project, such as the management of the organization that built the AI model, may also add their input and follow through to make sure it is considered.\nIf an organization is large, the AI model\u2019s supply chain can involve extended teams and even multiple organizations. Or, it is entirely possible that by using cloud services and AI marketplaces, a single developer can perform all of these functions alone. In any case, you can add an enterprise blockchain technology, like Hyperledger Fabric, to the stack so you can track, trace, audit, and even recall your model.\nAn enterprise blockchain network is sometimes used to bring transparency to the supply chain. This helps network participants trust one another because they are members of the same blockchain network. The blockchain network is also really helpful when something goes wrong and a product needs to be quickly traced to its origin.\nIn the case of Walmart, they pioneered the use of enterprise blockchain to track and trace food that potentially carried a foodborne illness. For example, if a customer became sick from a package of sliced mangoes in any Walmart store, the mangoes had to be discarded at all of the stores because it took more than 6 days to trace the affected shipment. The new blockchain network cut this time to 2.2 seconds, saving Walmart the expense of discarding good mangoes. Walmart continues with their supply chain blockchain strategy today, which has become the foundation of automated payment systems for their many suppliers.\nWhen this strategy is applied to AI\u2019s opaque box, the convenience of a supply chain blockchain network will help you to track and trace important factors like the reason why an AI model\u2019s intent or domain has drifted, or to learn what type of treatment was given to data that was used to produce a certain outcome. As explained in the O\u2019Reilly book I co-authored, Blockchain Tethered AI, there are four blockchain controls for AI, which are:\nControl 1, which is pre-establishing identity and workflow criteria for people and systems, can be used with AI to verify that data and models have not undergone tampering or corruption. This control includes criteria for telling humans apart from AI models.Control 2 addresses distributing tamper-evident verification, which can be used with AI to make sure that the right people, systems, or intelligent agents\u2014with the right authorization\u2014are the only ones that participate in governance of or modification to the AI. This control can be used to create a tamper-evident audit trail of training data, even if that data is supplied by users in the form of chat history, as is the case with ChatGPT. A record can be stored on blockchain indicating whether the user has agreed to have their chat history used as training data or not, and if a chat is used as training data, the prompts within it can be reviewed by a human or intelligent agent for issues such as ethics violations, data sabotage, or other issues before it is used.Control 3 involves governing, instructing, and inhibiting intelligent agents, and will become very important when wanting to trace or reverse AI, or prove in court that the output of AI is traceable to certain people or organizations. Reviews and ratings of how a model is performing can help to catch and address inappropriate or unethical output.Control 4 is showing authenticity through user-viewable provenance, which will be especially important in using branded AI that has underlying components which come from distributed marketplaces. Understanding and proving provenance is a factor in legal issues involving AI.\nThis ability to track and trace can also be extended to the consumer, through the display of trust logos. The concept of trust logos, which are the long-time hallmark of e-commerce security, can be applied to AI by connecting the logos to the underlying blockchain network, and programming them to alert consumers should the AI model become compromised. A similar method could be used to show whether a customer service representative is an AI or a human.\nKeep in mind that people in different roles may need different types of information in order to trust AI models. Depending on the perspective of the entity requesting the information, different levels of traceability could be desired. For example, a person answering their phone should be able to see an indicator as to whether a caller is AI, and whether or not the AI is from a trustworthy source. An engineer deciding whether or not to integrate AI components with their models would want a much deeper understanding of the supply chain, and a stakeholder might want to see if the reviews and comments are authentic and find out what is being done to address any recalls. This also brings up the question of a special handshake to enable AI models to trust one another and establish boundaries.\nEven though you might not know everything about your AI model, you can commit the facts you do know to blockchain. Develop an AI factsheet as described in Chapter 1 of Blockchain Tethered AI. If you have used models that you have downloaded from marketplaces, you can typically find an AI model card and data cards that provide basic facts about the materials you are using. Also, you can always document that a part of the model is indeed \u201copaque,\u201d and complete that part later once the details are known.\nYou can implement your blockchain network for your AI model\u2019s supply chain in the same way that enterprise blockchain networks are used by developers for other purposes. You only need to record cryptographic verifications on your blockchain network, while storing the actual components of the AI off-chain. The code that comes with Blockchain Tethered AI can help you to visualize and implement this architecture.\nThis blockchain verification, which works similar to a checksum you might see when downloading a file, can be checked against the model and its components at any point to see if they have undergone any tampering. This type of use of a blockchain network doesn\u2019t involve cryptocurrency or miners or use any unusually high amounts of energy to run, and should be thought of instead as a distributed text-based super-log that is automated by smart contracts.\nBeing able to track and trace goods in this way helps prevent sales of counterfeit goods, helps food companies to recall items quickly without having to throw everything away, and helps artists, musicians, and content creators be paid for their work. When applying these techniques and controls to make AI\u2019s opaque box explainable, your AI models will enjoy the competitive advantage of being trackable, traceable, controllable, and even stoppable.",
  "link": "https://www.oreilly.com/radar/ais-opaque-box-is-actually-a-supply-chain/"
 },
 {
  "title": "The Alignment Problem Is Not New",
  "content": "\u201cMitigating the risk of extinction from A.I. should be a global priority alongside other societal-scale risks, such as pandemics and nuclear war,\u201d according to a statement signed by more than 350 business and technical leaders, including the developers of today\u2019s most important AI platforms.\nAmong the possible risks leading to that outcome is what is known as \u201cthe alignment problem.\u201d Will a future super-intelligent AI share human values, or might it consider us an obstacle to fulfilling its own goals? And even if AI is still subject to our wishes, might its creators\u2014or its users\u2014make an ill-considered wish whose consequences turn out to be catastrophic, like the wish of fabled King Midas that everything he touches turn to gold? Oxford philosopher Nick Bostrom, author of the book Superintelligence, once posited as a thought experiment an AI-managed factory given the command to optimize the production of paperclips. The \u201cpaperclip maximizer\u201d comes to monopolize the world\u2019s resources and eventually decides that humans are in the way of its master objective.\nFar-fetched as that sounds, the alignment problem is not just a far future consideration. We have already created a race of paperclip maximizers. Science fiction writer Charlie Stross has noted that today\u2019s corporations can be thought of as \u201cslow AIs.\u201d And much as Bostrom feared, we have given them an overriding command: to increase corporate profits and shareholder value. The consequences, like those of Midas\u2019s touch, aren\u2019t pretty. Humans are seen as a cost to be eliminated. Efficiency, not human flourishing, is maximized.\nIn pursuit of this overriding goal, our fossil fuel companies continue to deny climate change and hinder attempts to switch to alternative energy sources, drug companies peddle opioids, and food companies encourage obesity. Even once-idealistic internet companies have been unable to resist the master objective, and in pursuing it have created addictive products of their own, sown disinformation and division, and resisted attempts to restrain their behavior.\nEven if this analogy seems far fetched to you, it should give you pause when you think about the problems of AI governance.\nCorporations are nominally under human control, with human executives and governing boards responsible for strategic direction and decision-making. Humans are \u201cin the loop,\u201d and generally speaking, they make efforts to restrain the machine, but as the examples above show, they often fail, with disastrous results. The efforts at human control are hobbled because we have given the humans the same reward function as the machine they are asked to govern: we compensate executives, board members, and other key employees with options to profit richly from the stock whose value the corporation is tasked with maximizing. Attempts to add environmental, social, and governance (ESG) constraints have had only limited impact. As long as the master objective remains in place, ESG too often remains something of an afterthought.\nMuch as we fear a superintelligent AI might do, our corporations resist oversight and regulation. Purdue Pharma successfully lobbied regulators to limit the risk warnings planned for doctors prescribing Oxycontin and marketed this dangerous drug as non-addictive. While Purdue eventually paid a price for its misdeeds, the damage had largely been done and the opioid epidemic rages unabated.\nWhat might we learn about AI regulation from failures of corporate governance?\nAIs are created, owned, and managed by corporations, and will inherit their objectives. Unless we change corporate objectives to embrace human flourishing, we have little hope of building AI that will do so.We need research on how best to train AI models to satisfy multiple, sometimes conflicting goals rather than optimizing for a single goal. ESG-style concerns can\u2019t be an add-on, but must be intrinsic to what AI developers call the reward function. As Microsoft CEO Satya Nadella once said to me, \u201cWe [humans] don\u2019t optimize. We satisfice.\u201d (This idea goes back to Herbert Simon\u2019s 1956 book Administrative Behavior.) In a satisficing framework, an overriding goal may be treated as a constraint, but multiple goals are always in play. As\u00a0I once described this theory of constraints, \u201cMoney in a business is like gas in your car. You need to pay attention so you don\u2019t end up on the side of the road. But your trip is not a tour of gas stations.\u201d Profit should be an instrumental goal, not a goal in and of itself. And as to our actual goals, Satya put it well in our conversation: \u201cthe moral philosophy that guides us is everything.\u201dGovernance is not a \u201conce and done\u201d exercise. It requires constant vigilance, and adaptation to new circumstances at the speed at which those circumstances change. You have only to look at the slow response of bank regulators to the rise of CDOs and other mortgage-backed derivatives in the runup to the 2009 financial crisis to understand that time is of the essence.\nOpenAI CEO Sam Altman has begged for government regulation, but tellingly, has suggested that such regulation apply only to future, more powerful versions of AI. This is a mistake. There is much that can be done right now.\nWe should require registration of all AI models above a certain level of power, much as we require corporate registration. And we should define current best practices in the management of AI systems and make them mandatory, subject to regular, consistent disclosures and auditing, much as we require public companies to regularly disclose their financials.\nThe work that Timnit Gebru, Margaret Mitchell, and their coauthors have done on the disclosure of training data (\u201cDatasheets for Datasets\u201d) and the performance characteristics and risks of trained AI models (\u201cModel Cards for Model Reporting\u201d) are a good first draft of something much like the Generally Accepted Accounting Principles (and their equivalent in other countries) that guide US financial reporting. Might we call them \u201cGenerally Accepted AI Management Principles\u201d? \nIt\u2019s essential that these principles be created in close cooperation with the creators of AI systems, so that they reflect actual best practice rather than a set of rules imposed from without by regulators and advocates. But they can\u2019t be developed solely by the tech companies themselves. In his book Voices in the Code, James G. Robinson (now Director of Policy for OpenAI) points out that every algorithm makes moral choices, and explains why those choices must be hammered out in a participatory and accountable process. There is no perfectly efficient algorithm that gets everything right. Listening to the voices of those affected can radically change our understanding of the outcomes we are seeking.\nBut there\u2019s another factor too. OpenAI has said that \u201cOur alignment research aims to make artificial general intelligence (AGI) aligned with human values and follow human intent.\u201d Yet many of the world\u2019s ills are the result of the difference between stated human values and the intent expressed by actual human choices and actions. Justice, fairness, equity, respect for truth, and long-term thinking are all in short supply. An AI model such as GPT4 has been trained on a vast corpus of human speech, a record of humanity\u2019s thoughts and feelings. It is a mirror. The biases that we see there are our own. We need to look deeply into that mirror, and if we don\u2019t like what we see, we need to change ourselves, not just adjust the mirror so it shows us a more pleasing picture!\nTo be sure, we don\u2019t want AI models to be spouting hatred and misinformation, but simply fixing the output is insufficient. We have to reconsider the input\u2014both in the training data and in the prompting. The quest for effective AI governance is an opportunity to interrogate our values and to remake our society in line with the values we choose. The design of an AI that will not destroy us may be the very thing that saves us in the end.",
  "link": "https://www.oreilly.com/radar/the-alignment-problem-is-not-new/"
 },
 {
  "title": "You Can\u2019t Regulate What You Don\u2019t Understand",
  "content": "The world changed on November 30, 2022 as surely as it did on August 12, 1908 when the first Model T left the Ford assembly line. That was the date when OpenAI released ChatGPT, the day that AI emerged from research labs into an unsuspecting world. Within two months, ChatGPT had over a hundred million users\u2014faster adoption than any technology in history.\nThe hand wringing soon began. Most notably, The Future of Life Institute published an open letter calling for an immediate pause in advanced AI research, asking: \u201cShould we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization?\u201d\nIn response, the Association for the Advancement of Artificial Intelligence published its own letter citing the many positive differences that AI is already making in our lives and noting existing efforts to improve AI safety and to understand its impacts. Indeed, there are important ongoing gatherings about AI regulation like the Partnership on AI\u2019s recent convening on Responsible Generative AI, which happened just this past week. The UK has already announced its intention to regulate AI, albeit with a light, \u201cpro-innovation\u201d touch. In the US, Senate Minority Leader Charles Schumer has announced plans to introduce \u201ca framework that outlines a new regulatory regime\u201d for AI. The EU is sure to follow, in the worst case leading to a patchwork of conflicting regulations.\nAll of these efforts reflect the general consensus that regulations should address issues like data privacy and ownership, bias and fairness, transparency, accountability, and standards. OpenAI\u2019s own AI safety and responsibility guidelines cite those same goals, but in addition call out what many people consider the central, most general question: how do we align AI-based decisions with human values? They write:\n\u201cAI systems are becoming a part of everyday life. The key is to ensure that these machines are aligned with human intentions and values.\u201d \nBut whose human values? Those of the benevolent idealists that most AI critics aspire to be? Those of a public company bound to put shareholder value ahead of customers, suppliers, and society as a whole? Those of criminals or rogue states bent on causing harm to others? Those of someone well meaning who, like Aladdin, expresses an ill-considered wish to an all-powerful AI genie?\nThere is no simple way to solve the alignment problem. But alignment will be impossible without robust institutions for disclosure and auditing. If we want prosocial outcomes, we need to design and report on the metrics that explicitly aim for those outcomes and measure the extent to which they have been achieved. That is a crucial first step, and we should take it immediately. These systems are still very much under human control. For now, at least, they do what they are told, and when the results don\u2019t match expectations, their training is quickly improved. What we need to know is what they are being told.\nWhat should be disclosed? There is an important lesson for both companies and regulators in the rules by which corporations\u2014which science-fiction writer Charlie Stross has memorably called \u201cslow AIs\u201d\u2014are regulated. One way we hold companies accountable is by requiring them to share their financial results compliant with Generally Accepted Accounting Principles or the International Financial Reporting Standards. If every company had a different way of reporting its finances, it would be impossible to regulate them.\nToday, we have dozens of organizations that publish AI principles, but they provide little detailed guidance. They all say things like\u00a0 \u201cMaintain user privacy\u201d and \u201cAvoid unfair bias\u201d but they don\u2019t say exactly under what circumstances companies gather facial images from surveillance cameras, and what they do if there is a disparity in accuracy by skin color. Today, when disclosures happen, they are haphazard and inconsistent, sometimes appearing in research papers, sometimes in earnings calls, and sometimes from whistleblowers. It is almost impossible to compare what is being done now with what was done in the past or what might be done in the future. Companies cite user privacy concerns, trade secrets, the complexity of the system, and various other reasons for limiting disclosures. Instead, they provide only general assurances about their commitment to safe and responsible AI. This is unacceptable.\nImagine, for a moment, if the standards that guide financial reporting simply said that companies must accurately reflect their true financial condition without specifying in detail what that reporting must cover and what \u201ctrue financial condition\u201d means. Instead, independent standards bodies such as the Financial Accounting Standards Board, which created and oversees GAAP, specify those things in excruciating detail. Regulatory agencies such as the Securities and Exchange Commission then require public companies to file reports according to GAAP, and auditing firms are hired to review and attest to the accuracy of those reports.\nSo too with AI safety. What we need is something equivalent to GAAP for AI and algorithmic systems more generally. Might we call it the Generally Accepted AI Principles? We need an independent standards body to oversee the standards, regulatory agencies equivalent to the SEC and ESMA to enforce them, and an ecosystem of auditors that is empowered to dig in and make sure that companies and their products are making accurate disclosures.\nBut if we are to create GAAP for AI, there is a lesson to be learned from the evolution of GAAP itself. The systems of accounting that we take for granted today and use to hold companies accountable were originally developed by medieval merchants for their own use. They were not imposed from without, but were adopted because they allowed merchants to track and manage their own trading ventures. They are universally used by businesses today for the same reason.\nSo, what better place to start with developing regulations for AI than with the management and control frameworks used by the companies that are developing and deploying advanced AI systems?\nThe creators of generative AI systems and Large Language Models already have tools for monitoring, modifying, and optimizing them. Techniques such as RLHF (\u201cReinforcement Learning from Human Feedback\u201d) are used to train models to avoid bias, hate speech, and other forms of bad behavior. The companies are collecting massive amounts of data on how people use these systems. And they are stress testing and \u201cred teaming\u201d them to uncover vulnerabilities. They are post-processing the output, building safety layers, and have begun to harden their systems against \u201cadversarial prompting\u201d and other attempts to subvert the controls they have put in place. But exactly how this stress testing, post processing, and hardening works\u2014or doesn\u2019t\u2014is mostly invisible to regulators.\nRegulators should start by formalizing and requiring detailed disclosure about the measurement and control methods already used by those developing and operating advanced AI systems.\nIn the absence of operational detail from those who actually create and manage advanced AI systems, we run the risk that regulators and advocacy groups\u00a0 \u201challucinate\u201d much like Large Language Models do, and fill the gaps in their knowledge with seemingly plausible but impractical ideas.\nCompanies creating advanced AI should work together to formulate a comprehensive set of operating metrics that can be reported regularly and consistently to regulators and the public, as well as a process for updating those metrics as new best practices emerge.\nWhat we need is an ongoing process by which the creators of AI models fully, regularly, and consistently disclose the metrics that they themselves use to manage and improve their services and to prohibit misuse. Then, as best practices are developed, we need regulators to formalize and require them, much as accounting regulations have formalized\u00a0 the tools that companies already used to manage, control, and improve their finances. It\u2019s not always comfortable to disclose your numbers, but mandated disclosures have proven to be a powerful tool for making sure that companies are actually following best practices.\nIt is in the interests of the companies developing advanced AI to disclose the methods by which they control AI and the metrics they use to measure success, and to work with their peers on standards for this disclosure. Like the regular financial reporting required of corporations, this reporting must be regular and consistent. But unlike financial disclosures, which are generally mandated only for publicly traded companies, we likely need AI disclosure requirements to apply to much smaller companies as well.\nDisclosures should not be limited to the quarterly and annual reports required in finance. For example, AI safety researcher Heather Frase has argued that \u201ca public ledger should be created to report incidents arising from large language models, similar to cyber security or consumer fraud reporting systems.\u201d There should also be dynamic information sharing such as is found in anti-spam systems.\nIt might also be worthwhile to enable testing by an outside lab to confirm that best practices are being met and what to do when they are not. One interesting historical parallel for product testing may be found in the certification of fire safety and electrical devices by an outside non-profit auditor, Underwriter\u2019s Laboratory. UL certification is not required, but it is widely adopted because it increases consumer trust.\nThis is not to say that there may not be regulatory imperatives for cutting-edge AI technologies that are outside the existing management frameworks for these systems. Some systems and use cases are riskier than others. National security considerations are a good example. Especially with small LLMs that can be run on a laptop, there is a risk of an irreversible and uncontrollable proliferation of technologies that are still poorly understood. This is what Jeff Bezos has referred to as a \u201cone way door,\u201d a decision that, once made, is very hard to undo. One way decisions require far deeper consideration, and may require regulation from without that runs ahead of existing industry practices. \nFurthermore, as Peter Norvig of the Stanford Institute for Human Centered AI noted in a review of a draft of this piece, \u201cWe think of \u2018Human-Centered AI\u2019 as having three spheres: the user (e.g., for a release-on-bail recommendation system, the user is the judge); the stakeholders (e.g., the accused and their family, plus the victim and family of past or potential future crime); the society at large (e.g. as affected by mass incarceration).\u201d\nPrinceton computer science professor Arvind Narayanan has noted that these systemic harms to society that transcend the harms to individuals require a much longer term view and broader schemes of measurement than those typically carried out inside corporations. But despite the prognostications of groups such as the Future of Life Institute, which penned the AI Pause letter, it is usually difficult to anticipate these harms in advance. Would an \u201cassembly line pause\u201d in 1908 have led us to anticipate the massive social changes that 20th century industrial production was about to unleash on the world? Would such a pause have made us better or worse off?\nGiven the radical uncertainty about the progress and impact of AI, we are better served by mandating transparency and building institutions for enforcing accountability than we are in trying to head off every imagined particular harm.\nWe shouldn\u2019t wait to regulate these systems until they have run amok. But nor should regulators overreact to AI alarmism in the press. Regulations should first focus on disclosure of current monitoring and best practices. In that way, companies, regulators, and guardians of the public interest can learn together how these systems work, how best they can be managed, and what the systemic risks really might be.",
  "link": "https://www.oreilly.com/radar/you-cant-regulate-what-you-dont-understand/"
 },
 {
  "title": "ChatGPT, Now with Plugins",
  "content": "A few months ago, I wrote about some experiments with prime numbers.\u00a0I generated a 16-digit non-prime number by multiplying two 8-digit prime numbers, and asked ChatGPT (using GPT -3.5) whether the larger number was prime.\u00a0It answered correctly that the number was non-prime, but when it told me the number\u2019s prime factors, it was clearly wrong. It also generated a short program that implemented the widely used Miller-Rabin primality test. After fixing some obvious errors, I ran the program\u2013and while it told me (correctly) that my number was non-prime, when compared to a known good implementation of Miller-Rabin, ChatGPT\u2019s code made many mistakes. When it became available, GPT-4 gave me similar results. And the result itself\u2013well, that could have been a good guess. There\u2019s a roughly a 97% chance that a randomly chosen 16-digit number will be non-prime.\nOpenAI recently opened their long-awaited Plugins feature to users of ChatGPT Plus (the paid version) using the GPT-4 model.\u00a0One of the first plugins was from Wolfram, the makers of Mathematica and Wolfram Alpha. I had to try this! Specifically, I was compelled to re-try my prime test. And everything worked: ChatGPT sent the problem to Wolfram, it determined that number was not prime, and gave me the correct prime factors. It didn\u2019t generate any code, but provided a link to the Wolfram Alpha result page that described how to test for primality. The process of going through ChatGPT to Wolfram and back was also painfully slow, much slower than using Wolfram Alpha directly or writing a few lines of Python. But it worked and, for fans of prime numbers, that\u2019s a plus.\nI was still uncomfortable.\u00a0How does ChatGPT decide what to offload to Wolfram Alpha, and what to handle on its own? I tried a few questions from calculus; unsurprisingly, they went to Wolfram.\u00a0Then I got really simple: \u201cHow much is 3 + 5?\u201d\u00a0 No Wolfram, and I wasn\u2019t surprised when ChatGPT told me the answer was 8.\u00a0But that begged the question: what about more complex arithmetic? So I asked \u201cHow much is 123456789 + 98776543321?\u201d, a problem that could be solved by any elementary school student who has learned how to carry. Again, no Wolfram, but this time, the answer was incorrect.\nWe\u2019ve long known that ChatGPT was poor at arithmetic, in addition to being poor at more advanced math. The Wolfram plugin solves the math problem with ease. However, ChatGPT is still poor at arithmetic, and still attempts to do arithmetic on its own. The important question that I can\u2019t answer is \u201cwhen does a problem become complex enough to send to the plugin?\u201d The plugin is a big win, but not an unqualified one.\nChatGPT\u2019s tendency to make up citations is another well-known problem. A few weeks ago, a story circulated about a lawyer who used ChatGPT to write a brief. ChatGPT cited a lot of case law, but made up all the citations. When a judge asked him to produce the actual case law, the lawyer went back to ChatGPT\u2013which obediently made up the cases themselves. The judge was not pleased. That raises another question: ChatGPT has always been prone to making up citations\u2013but now there\u2019s a plugin for that! The ScholarAI plugin searches academic databases for citations, and returns links. That wouldn\u2019t have helped this lawyer (I don\u2019t yet see plugins from Westlaw or LexisNexis), but it\u2019s worth asking: what about citations?\nI first tried asking a medical question. I\u2019m not a doctor, so the question was simple: what\u2019s the latest research on antibiotic-resistant bacteria? ChatGPT sent the question to ScholarAI, and I got back a long list of relevant citations. (The plugin appeared to get into a loop, so I eventually terminated the output.) While I\u2019m not competent to evaluate the quality or relevance of the papers, all the links were valid: the papers were real, and the author names were correct. No hallucinations here.\nI followed up with some questions about English literature (I have a PhD, so I can make up real questions). I didn\u2019t get as many citations in return, possibly because we don\u2019t have preprint servers like ArXiv, and have done little to protest journals\u2019 proprietary lock on scholarship. However, the citations I got were valid: real books and articles, with the authors listed correctly.\nThat begged another question, though. A list of articles is certainly useful, but you still have to read them all to write the paper. Could ChatGPT write an essay for me?\u00a0 I asked it to write about colonialism in the work of Salman Rushdie, and got a passable short essay. It is what I\u2019d call a \u201clazy\u201d prompt: what I\u2019d expect from a student who was interested in getting out of work, rather than using the AI to learn. There were citations, and they were real; ChatGPT didn\u2019t link to the publications cited, but Google made it easy to find them. The resulting essay didn\u2019t demonstrate any familiarity with the articles beyond the abstract\u2013fair enough, since for most of the sources, the abstract was all that was publicly available. More to the point, the article didn\u2019t really make any connections to Rushdie\u2019s fiction. There were many sentences like this: \u201cHamish Dalley discusses the role of the historical novel in postcolonial writing, a genre to which many of Rushdie\u2019s works belong.\u201d True, but that doesn\u2019t say much about either Rushdie\u2019s work or Dalley\u2019s. As I said, the essay was passable, but if I had to grade it, the student who turned it in wouldn\u2019t have been happy. Still, ChatGPT and ScholarAI get credit for doing a decent literature search that could be the basis for an excellent paper. And if a student took this initial prompt, read the academic articles along with Rushdie\u2019s novels, and used that to write a more detailed prompt telling ChatGPT exactly what points he wanted to make, with relevant quotations, the result could have been excellent. An essay isn\u2019t an exercise in providing N*1000 words; it\u2019s the outcome of a thought process that involves engaging with the subject matter. If ChatGPT and ScholarAI facilitate that engagement, I wouldn\u2019t object. But let\u2019s be clear: regardless of who generates the words, ChatGPT\u2019s users still have to do the reading and thinking.\nAs with the Wolfram plugin, it\u2019s helpful to understand when ChatGPT is using ScholarAI, and when it isn\u2019t. I asked ChatGPT to find articles by me; when using the plugin, it couldn\u2019t find any, although it apologetically gave me a list of articles whose authors had the first name Michael. The sad list of Michael-authored articles notwithstanding, I\u2019ll count that response as \u201ccorrect.\u201d\u00a0I haven\u2019t published any academic papers, though I have published a lot on O\u2019Reilly Radar\u2013material that any web search can find, without the need for AI or the risk of hallucination.\nIf you dig a bit deeper, the results are puzzling. If you use ChatGPT with plugins enabled and write a prompt that tells it not to use the plugin, it comes up empty, but suggests that you research online databases like Google Scholar. If you start a new conversation and do not enable plugins (plugins can only be enabled or disabled at the start of a conversation), you still get nothing\u2013but ChatGPT does tell you that Michael Loukides is a well-known author who has frequently written for O\u2019Reilly, and to check on the O\u2019Reilly website for articles. (It isn\u2019t clear whether these different responses have to do with the state of the plugin, or the way ChatGPT randomizes its output.)\u00a0Flattery will get you somewhere, I suppose, but not very far. My publication history with O\u2019Reilly goes back to the 1990s, and is all public; it\u2019s not clear why ChatGPT is unaware of it. Starting a new conversation with Bing searches enabled got me a list of valid links to articles that I\u2019ve written\u2013but I shouldn\u2019t have had to try three times, the process was much slower than searching with Bing (or Google) directly, and it wasn\u2019t clear why some articles were included and some weren\u2019t. And you really do have to try multiple times: you can\u2019t use both Bing searches and plugins in the same conversation.\nAs with the Wolfram plugin, ScholarAI is a big improvement\u2013but again, not an unqualified one. You still have to know whether the content you\u2019re looking for is in an academic journal, on the web, or somewhere else. While ChatGPT tells you when it is using a plugin, and which plugin it is using, you can\u2019t always predict what it will do in advance\u2013and when it doesn\u2019t use a plugin, ChatGPT is vulnerable to the same errors we\u2019ve come to expect. You still have to experiment, and you still have to check the results.\nAs another test, I used the Kayak plugin to check out flights for some trips I might take. The plugin does a good job with major airports (including smaller ones), though it seemed to be hit-or-miss with very small airports, like New Haven (HVN). That\u2019s a limitation of Kayak, rather than the plugin itself or ChatGPT. You currently have to enable the plugins you\u2019re going to use at the start of each conversation, and ChatGPT doesn\u2019t allow you to enable competing plugins. You can install both Kayak and Expedia, but you can only use one in any chat. I wouldn\u2019t be surprised if this behavior changes as plugins mature.\nFinally: all the plugins I installed were free of charge. However, I don\u2019t think it\u2019s called the \u201cplugin store\u201d for nothing. It wouldn\u2019t surprise me to see charges for plugins, and I would be surprised if some plugins eventually require a subscription to a paid account. A number of the plugins access subscription-based services; I expect that subscriptions will be required once we are out of the Beta period.\nI\u2019m excited that plugins have finally arrived. Plugins are still in beta, so their behavior will almost certainly change; the behaviors I\u2019ve described may have changed by the time you read this. Several changed while I was writing this article. Plugins certainly don\u2019t eliminate the need to be careful about hallucinations and other kinds of errors, nor do they replace the need for thinking. But it\u2019s hard to understate how important it is that ChatGPT can now reach out and access current data. When ChatGPT was limited to data before November 2021, it was an intriguing toy. It\u2019s looking more and more like a tool. ",
  "link": "https://www.oreilly.com/radar/chatgpt-now-with-plugins/"
 }
]